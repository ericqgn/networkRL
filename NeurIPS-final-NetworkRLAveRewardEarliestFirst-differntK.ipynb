{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from scipy import special\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalNetwork:\n",
    "    def __init__(self, nodeNum, k):\n",
    "        self.nodeNum = nodeNum #the total number of nodes in this network\n",
    "        self.adjacencyMatrix = np.eye(self.nodeNum, dtype = int) #initialize the adjacency matrix of the global network\n",
    "        self.k = k #the number of hops used in learning\n",
    "        self.adjacencyMatrixPower = [np.eye(self.nodeNum, dtype = int)] #cache the powers of the adjacency matrix\n",
    "        self.neighborDict = {} #use a hashmap to store the ((node, dist), neighbors) pairs which we have computed\n",
    "        self.addingEdgesFinished = False #have we finished adding edges?\n",
    "    \n",
    "    #add an undirected edge between node i and j\n",
    "    def addEdge(self, i, j):\n",
    "        self.adjacencyMatrix[i, j] = 1\n",
    "        self.adjacencyMatrix[j, i] = 1\n",
    "    \n",
    "    #finish adding edges, so we can construct the k-hop neighborhood after adding edges\n",
    "    def finishAddingEdges(self):\n",
    "        temp = np.eye(self.nodeNum, dtype = int)\n",
    "        #the d-hop adjacency matrix is stored in self.adjacencyMatrixPower[d]\n",
    "        for _ in range(self.k):\n",
    "            temp = np.matmul(temp, self.adjacencyMatrix)\n",
    "            self.adjacencyMatrixPower.append(temp)\n",
    "        self.addingEdgesFinished = True\n",
    "        print(self.adjacencyMatrixPower)\n",
    "    \n",
    "    #query the d-hop neighborhood of node i, return a list of node indices.\n",
    "    def findNeighbors(self, i, d):\n",
    "        if not self.addingEdgesFinished:\n",
    "            print(\"Please finish adding edges before call findNeighbors!\")\n",
    "            return -1\n",
    "        if (i, d) in self.neighborDict: #if we have computed the answer before, return it\n",
    "            return self.neighborDict[(i, d)]\n",
    "        neighbors = []\n",
    "        for j in range(self.nodeNum):\n",
    "            if self.adjacencyMatrixPower[d][i, j] > 0: #this element > 0 implies that dist(i, j) <= d\n",
    "                neighbors.append(j)\n",
    "        self.neighborDict[(i, d)] = neighbors #cache the answer so we can reuse later\n",
    "        return neighbors\n",
    "\n",
    "class AccessNetwork(GlobalNetwork):\n",
    "    def __init__(self, nodeNum, k, accessNum):\n",
    "        super(AccessNetwork, self).__init__(nodeNum, k)\n",
    "        self.accessNum = accessNum\n",
    "        self.accessMatrix = np.zeros((nodeNum, accessNum), dtype = int)\n",
    "    \n",
    "    #add an access point a for node i\n",
    "    def addAccess(self, i, a):\n",
    "        self.accessMatrix[i, a] = 1\n",
    "        \n",
    "    #finish adding access points. we can construct the neighbor graph\n",
    "    def finishAddingAccess(self):\n",
    "        #use accessMatrix to construct the adjacency matrix of (user) nodes\n",
    "        self.adjacencyMatrix = np.matmul(self.accessMatrix, np.transpose(self.accessMatrix))\n",
    "        \n",
    "        #calculate the number of users sharing each access point\n",
    "        self.numNodePerAccess = np.sum(self.accessMatrix,axis = 0)\n",
    "        \n",
    "        super(AccessNetwork, self).finishAddingEdges()\n",
    "    \n",
    "    #find the access points for node i\n",
    "    def findAccess(self, i):\n",
    "        accessPoints = []\n",
    "        for j in range(self.accessNum):\n",
    "            if self.accessMatrix[i, j] > 0:\n",
    "                accessPoints.append(j)\n",
    "        return accessPoints\n",
    "    def setTransmitProb(self,transmitProb):\n",
    "        self.transmitProb = transmitProb\n",
    "        \n",
    "\n",
    "class Node:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        self.state = [] #The list of local state at different time steps\n",
    "        self.action = [] #The list of local actions at different time steps\n",
    "        self.reward = [] #The list of local actions at different time steps\n",
    "        self.currentTimeStep = 0 #Record the current time step.\n",
    "        self.paramsDict = {} #use a hash map to query the parameters given a state (or neighbors' states)\n",
    "        self.QDict = {} #use a hash map to query to the Q value given a (state, action) pair\n",
    "        self.kHop = [] #The list to record the (state, action) pairs of k-hop neighbors\n",
    "    #get the local Q at timeStep\n",
    "    def getQ(self, kHopStateAction):\n",
    "        #if the Q value of kHopStateAction hasn't been queried before, return 0.0 (initial value)\n",
    "        return self.QDict.get(kHopStateAction, 0.0)\n",
    "    \n",
    "    #initialize the local state\n",
    "    def initializeState(self):\n",
    "        pass\n",
    "    #update the local state, it may depends on the states of other nodes at the last time step.\n",
    "    #Remember to increase self.currentTimeStep by 1\n",
    "    def updateState(self):\n",
    "        pass\n",
    "    #update the local action\n",
    "    def updateAction(self):\n",
    "        pass\n",
    "    #update the local reward\n",
    "    def updateReward(self):\n",
    "        pass\n",
    "    #update the local Q value\n",
    "    def updateQ(self):\n",
    "        pass\n",
    "    #update the local parameter\n",
    "    def updateParams(self):\n",
    "        pass\n",
    "    #clear the record. Called when a new inner loop starts. \n",
    "    def restart(self, clearPolicy = True):\n",
    "        self.state.clear()\n",
    "        self.action.clear()\n",
    "        self.reward.clear()\n",
    "        if clearPolicy == True:\n",
    "            self.paramsDict.clear()\n",
    "        \n",
    "        self.kHop = []\n",
    "        self.currentTimeStep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructLinearNetwork(nodeNum, k, nodePerBlock , transmitProb = 'allone'):\n",
    "    #compute the number of access points\n",
    "    accessNum = (nodeNum - 1)//nodePerBlock\n",
    "        \n",
    "    if accessNum <= 0:\n",
    "        print(\"nodeNum is not large enough!\")\n",
    "        return null\n",
    "    accessNetwork = AccessNetwork(nodeNum = nodeNum, k = k, accessNum = accessNum)\n",
    "    for i in range(nodeNum):\n",
    "        j = i//nodePerBlock\n",
    "        if j >= 1:\n",
    "            accessNetwork.addAccess(i, j - 1)\n",
    "        if j < accessNum:\n",
    "            accessNetwork.addAccess(i, j)\n",
    "    accessNetwork.finishAddingAccess()\n",
    "    \n",
    "    \n",
    "    # setting transmitProb \n",
    "    if transmitProb == 'allone':\n",
    "        transmitProb = np.ones(accessNum)\n",
    "    elif transmitProb == 'random':\n",
    "        transmitProb = np.random.rand(accessNum)\n",
    "    \n",
    "    accessNetwork.setTransmitProb(transmitProb)\n",
    "    return accessNetwork\n",
    "\n",
    "def constructGridNetwork(nodeNum, width, height, k, nodePerGrid, transmitProb = 'allone'):\n",
    "    if nodeNum != width * height * nodePerGrid:\n",
    "        print(\"nodeNum does not satisfy the requirement of grid network!\", nodeNum, width, height, nodePerGrid)\n",
    "        return null\n",
    "    accessNum = (width - 1) * (height - 1)\n",
    "    accessNetwork = AccessNetwork(nodeNum = nodeNum, k = k, accessNum = accessNum)\n",
    "    for j in range(accessNum):\n",
    "        upperLeft = j//(width - 1) * width + j%(width - 1)\n",
    "        upperRight = upperLeft + 1\n",
    "        lowerLeft = upperLeft + width\n",
    "        lowerRight = lowerLeft + 1\n",
    "        for a in [upperLeft, upperRight, lowerLeft, lowerRight]:\n",
    "            for b in range(nodePerGrid):\n",
    "                accessNetwork.addAccess(nodePerGrid * a + b, j)\n",
    "    accessNetwork.finishAddingAccess()\n",
    "    \n",
    "    # setting transmitProb \n",
    "    if transmitProb == 'allone':\n",
    "        transmitProb = np.ones(accessNum)\n",
    "    elif transmitProb == 'random':\n",
    "        transmitProb = np.random.rand(accessNum)\n",
    "    \n",
    "    accessNetwork.setTransmitProb(transmitProb)\n",
    "\n",
    "    return accessNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class accessNodeAve(Node):\n",
    "    def __init__(self, index, ddl, arrivalProb, accessNetwork):\n",
    "        super(accessNodeAve, self).__init__(index)\n",
    "        self.ddl = ddl #the initial deadline of each packet\n",
    "        self.arrivalProb = arrivalProb #the arrival probability at each timestep\n",
    "        self.gamma = 1.0 #the discount factor\n",
    "        #we use packetQueue to represent the current local state, which is (e_1, e_2, ..., e_d)\n",
    "        self.packetQueue = np.zeros(self.ddl, dtype = int) #use 1 to represent a packet with this remaining time, otherwise 0\n",
    "        self.accessPoints = accessNetwork.findAccess(i=index) #find and cache the access points this node can access\n",
    "        self.accessNum = len(self.accessPoints) #the number of access points\n",
    "        self.actionNum = self.accessNum  + 1 #the number of possible actions\n",
    "        self.stateNum = 2** self.ddl # number of possible states\n",
    "        \n",
    "        self.mu = 0.0 # estimate for average reward\n",
    "        self.dummyState = tuple(np.zeros(self.ddl, dtype = int))\n",
    "        self.dummyAction = -1\n",
    "        #print('self.dummyStateAction' , self.dummyStateAction)\n",
    "        #construct a list of possible actions\n",
    "        self.actionList = [-1] #(-1, -1) is an empty action that does nothing\n",
    "        for a in self.accessPoints:\n",
    "            self.actionList.append( a)\n",
    "    #remove the first element in packetQueue, and add packetState to the end\n",
    "    def rotateAdd(self, packetState):\n",
    "        #print('self.packetQueue[1:] =',self.packetQueue[1:],'self.ddl = ',self.ddl, 'packetState = ',packetState )\n",
    "        self.packetQueue = np.insert(self.packetQueue[1:], self.ddl - 1, packetState)\n",
    "        #print('new = ',self.packetQueue)\n",
    "        \n",
    "    #initialize the local state (called at the beginning of the training process)\n",
    "    def initializeState(self):\n",
    "        newPacketState = np.random.binomial(1, self.arrivalProb) #Is there a packer arriving at time step 0?\n",
    "        self.rotateAdd(newPacketState) #get the packet queue at time step 0\n",
    "        self.state.append(tuple(self.packetQueue)) #append this state to state record\n",
    "    \n",
    "    #At each time step t, call updateState, updateAction, updateReward, updateQ in this order\n",
    "    def updateState(self):\n",
    "        self.currentTimeStep += 1\n",
    "        lastAction = self.action[-1]\n",
    "        \n",
    "        # find the earliest slot\n",
    "        nonEmptySlots = np.nonzero(self.packetQueue == 1)\n",
    "        \n",
    "        if len(nonEmptySlots) >0: # queue not empty\n",
    "            #if the reward at the last time step is positive, we have successfully send out a packet\n",
    "            if self.reward[-1] > 1 - 1e-3:\n",
    "                self.packetQueue[nonEmptySlots[0]] = 0 # earliest packet is sent\n",
    "        \n",
    "        #sample whether next packet comes\n",
    "        newPacketState = np.random.binomial(1, self.arrivalProb) #Is there a packer arriving at time step 0?\n",
    "        self.rotateAdd(newPacketState) #get the packet queue at time step 0\n",
    "        self.state.append(tuple(self.packetQueue)) #append this state to state record\n",
    "        if len(self.state)>2:\n",
    "            self.state.pop(0)# only keep current\n",
    "            \n",
    "        \n",
    "    def updateAction(self):\n",
    "        # get the current state\n",
    "        currentState = tuple(self.packetQueue)\n",
    "        # fetch the params based on the current state. If haven't updated before, return all zeros\n",
    "        params = self.paramsDict.get(currentState, np.zeros(self.actionNum))\n",
    "        # compute the probability vector\n",
    "        probVec = special.softmax(params)\n",
    "        # randomly select an action based on probVec\n",
    "        currentAction = self.actionList[np.random.choice(a = self.actionNum, p = probVec)]\n",
    "        \n",
    "        self.action.append(currentAction)\n",
    "        if(len(self.action)>2):\n",
    "            self.action.pop(0) # through away the first one. Only keep current and last action\n",
    "    \n",
    "    #oneHopNeighbors is a list of accessNodes\n",
    "    def updateReward(self, oneHopNeighbors, accessNetwork):\n",
    "        #decide if a packet is successfully sending out\n",
    "        currentAction = self.action[-1]\n",
    "        if currentAction == -1: # the do nothing action\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "        currentState = np.array(self.state[-1])\n",
    "        \n",
    "        #check if the node try to send out an empty slot\n",
    "        if np.all(currentState == 0): # if the current queue is empty\n",
    "            # zero reward\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "\n",
    "        \n",
    "        for neighbor in oneHopNeighbors:\n",
    "            if neighbor.index == self.index:\n",
    "                continue\n",
    "            neighborAction = neighbor.action[-1]\n",
    "            \n",
    "            if neighborAction != currentAction: \n",
    "                continue\n",
    "            else:\n",
    "                neighborState = np.array(neighbor.state[-1])\n",
    "                #print('neighborState', neighborState)\n",
    "                if np.any(neighborState == 1): # neighbor queue non empty, conflict!\n",
    "                    #print('conflict!')\n",
    "                    self.reward.append(0.0)\n",
    "                    if(len(self.reward)>2):\n",
    "                        self.reward.pop(0)\n",
    "                    return\n",
    "        \n",
    "        # no conflict, send\n",
    "        transmitSuccess = np.random.binomial(1, accessNetwork.transmitProb[currentAction])\n",
    "        if transmitSuccess == 1:\n",
    "            self.reward.append(1.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "        else:\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "\n",
    "    def isDummyStateAction(self,state,action):\n",
    "        if state == self.dummyState and action == self.dummyAction:\n",
    "            #print(self.dummyState,'==',state,', ',self.dummyAction,'==',action)\n",
    "            return True\n",
    "        else:\n",
    "            #print(self.dummyState,'!=',state,', ',self.dummyAction,'!=',action)\n",
    "            return False\n",
    "                                \n",
    "    \n",
    "    #kHopNeighbors is a list of accessNodes, alpha is learning rate\n",
    "    def updateQ(self, kHopNeighbors, alpha):\n",
    "        lastStateAction = []\n",
    "        currentStateAction = []\n",
    "        #construct a list of the state-action pairs of k-hop neighbors\n",
    "        dummyFlag = []\n",
    "        for neighbor in kHopNeighbors:\n",
    "            neighborLastState = neighbor.state[-2]\n",
    "            neighborCurrentState = neighbor.state[-1]\n",
    "            neighborLastAction = neighbor.action[-2]\n",
    "            neighborCurrentAction = neighbor.action[-1]\n",
    "            lastStateAction.append((neighborLastState, neighborLastAction))\n",
    "            currentStateAction.append((neighborCurrentState, neighborCurrentAction))\n",
    "            dummyFlag.append(neighbor.isDummyStateAction(neighborLastState,neighborLastAction))\n",
    "        lastStateAction = tuple(lastStateAction)\n",
    "        currentStateAction = tuple(currentStateAction)\n",
    "        #fetch the Q value based on neighbors' states and actions\n",
    "        lastQTerm1 = self.QDict.get(lastStateAction, 0.0)\n",
    "        lastQTerm2 = self.QDict.get(currentStateAction, 0.0)\n",
    "        #compute the temporal difference\n",
    "        \n",
    "        self.mu = (1-alpha) * self.mu + alpha* self.reward[-1] # compute new mu\n",
    "        \n",
    "        \n",
    "        temporalDiff = self.reward[-2] - self.mu+  lastQTerm2 - lastQTerm1\n",
    "        #print('lastStateAction',lastStateAction)\n",
    "        #perform the Q value update when last state action is not dummy\n",
    "        if not all(dummyFlag):\n",
    "            self.QDict[lastStateAction] = lastQTerm1 + alpha * temporalDiff\n",
    "            #print(self.QDict[lastStateAction])\n",
    "        # if this time step 1, we should also put lastStateAction into history record\n",
    "        \n",
    "        #if len(self.kHop) == 0:\n",
    "            #self.kHop.append(lastStateAction)\n",
    "        #put currentStateAction into history record\n",
    "        self.kHop = currentStateAction\n",
    "    \n",
    "    #eta is the learning rate\n",
    "    def updateParams(self, kHopNeighbors, eta):\n",
    "        #for t = 0, 1, ..., T, compute the term in g_{i, t}(m) before \\nabla\n",
    "        mutiplier1 = 0.0\n",
    "        t = self.currentTimeStep\n",
    "        #print(' t = ', t,', khop = ', self.kHop)\n",
    "        for neighbor in kHopNeighbors:\n",
    "            neighborKHop = neighbor.kHop\n",
    "            neighborQ = neighbor.getQ(neighborKHop)\n",
    "            mutiplier1 += neighborQ\n",
    "        \n",
    "        mutiplier1 /= nodeNum\n",
    "        #finish constructing mutiplier1\n",
    "        #compute the gradient with respect to the parameters associated with s_i(t)\n",
    "        currentState = self.state[-1]\n",
    "        currentAction = self.action[-1]\n",
    "        \n",
    "        defaultPolicy = np.zeros(self.actionNum)\n",
    "        defaultPolicy[0]=2\n",
    "        \n",
    "        # for test only!!!\n",
    "#         if self.index == 0:\n",
    "#             defaultPolicy[1]=-1\n",
    "        # End of Test!!!\n",
    "        \n",
    "        \n",
    "        params = self.paramsDict.get(currentState, defaultPolicy)\n",
    "        probVec = special.softmax(params)\n",
    "        grad = -probVec\n",
    "        actionIndex = self.actionList.index(currentAction) #get the index of currentAction\n",
    "        grad[actionIndex] += 1.0\n",
    "        self.paramsDict[currentState] = params + eta * mutiplier1 * grad\n",
    "        #print('t=',t ,'  id = ', self.index, '  current state=', len(currentState),' grad = ',grad, 'mutiplier1 =', mutiplier1, 'params=', params)\n",
    "\n",
    "    def setBenchmarkPolicy(self,accessNetwork): # set a naive benchmarkPolicy\n",
    "        proportionAction = []\n",
    "        for actionCounter in range(self.actionNum):\n",
    "            if self.actionList[actionCounter] == -1:\n",
    "                proportionAction.append(np.log(100*.11/4.0))\n",
    "            else:\n",
    "                numNodePerAccess = float(accessNetwork.numNodePerAccess[self.actionList[actionCounter]])\n",
    "                transmitProb = float(accessNetwork.transmitProb[self.actionList[actionCounter]])\n",
    "                print('numNodePerAccess = ',numNodePerAccess,' transmitProb = ',transmitProb)\n",
    "                proportionAction.append( np.log(100*transmitProb/numNodePerAccess))\n",
    "            \n",
    "        \n",
    "        for stateInt in range(self.stateNum): # enumerate state\n",
    "            currentState = self.int2state(stateInt) # turn state integer into binary list\n",
    "            actionParams = np.ones(self.actionNum,dtype = float) * (-10) # default to be all negative\n",
    "            \n",
    "            \n",
    "            if np.all( currentState == 0): # no packet in queue\n",
    "                actionParams[0] = 10.0 # do nothing\n",
    "            else:\n",
    "                actionParams = np.array(proportionAction) # proportional action\n",
    "            # update paramsDict\n",
    "            self.paramsDict[tuple(currentState)] = actionParams\n",
    "                \n",
    "            \n",
    "        \n",
    "    def int2state(self,stateInt):\n",
    "        currentState = np.zeros(self.ddl,dtype = int)\n",
    "        stateIntIterate = stateInt\n",
    "        for i in range(ddl):\n",
    "            currentState[i] = stateIntIterate% self.ddl\n",
    "            stateIntIterate = stateIntIterate//self.ddl\n",
    "        return currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAccessNetworkRL:\n",
    "    def __init__(self,ddl = 2, graphType = 'line', nodeNum = 10, maxK = 3 ,arrivalProb = None, transmitProb = 'random', gridW = 1, gridH = 1):\n",
    "        self.ddl = ddl\n",
    "        self.nodeNum = nodeNum\n",
    "        self.maxK = maxK\n",
    "        if(arrivalProb == None):\n",
    "            self.arrivalProb = np.random.rand(nodeNum)\n",
    "        else:\n",
    "            self.arrivalProb = arrivalProb\n",
    "            \n",
    "        if graphType == 'line':\n",
    "            self.accessNetwork = constructLinearNetwork(nodeNum = nodeNum, nodePerBlock = 1, k = maxK,transmitProb= transmitProb)\n",
    "        else:\n",
    "            self.accessNetwork =  constructGridNetwork(nodeNum = nodeNum, width = gridW, height = gridH, k = maxK, nodePerGrid = 1,transmitProb= transmitProb)\n",
    "            \n",
    "        self.nodeList = []\n",
    "        for i in range(nodeNum):\n",
    "            self.nodeList.append(accessNodeAve(index = i, ddl = ddl, arrivalProb = self.arrivalProb[i], accessNetwork = self.accessNetwork) )\n",
    "       \n",
    "    def train(self, k = 1, M = 10000, evalInterval = 500, restartIntervalQ = 50, restartIntervalPolicy = 50, clearPolicy = True):\n",
    "        for i in range(self.nodeNum):\n",
    "            self.nodeList[i].restart(clearPolicy)\n",
    "            self.nodeList[i].initializeState()\n",
    "            self.nodeList[i].updateAction()\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            neighborList = []\n",
    "            for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                neighborList.append(self.nodeList[j])\n",
    "            self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "            \n",
    "        \n",
    "        policyRewardList = []\n",
    "        policyRewardSmooth = []\n",
    "        policyRewardMuSmooth = []\n",
    "        policyRewardMu = []\n",
    "        for m in trange(M):\n",
    "            tmpReward = 0.0\n",
    "            tmpRewardMu = 0.0\n",
    "            for i in range(self.nodeNum): #update state-action\n",
    "                self.nodeList[i].updateState()\n",
    "                self.nodeList[i].updateAction()\n",
    "            \n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "                \n",
    "                tmpReward += self.nodeList[i].reward[-1] # add latest reward\n",
    "                tmpRewardMu += self.nodeList[i].mu\n",
    "                \n",
    "            policyRewardList.append(tmpReward/self.nodeNum)\n",
    "            policyRewardMu.append(tmpRewardMu/self.nodeNum)\n",
    "            \n",
    "            if m%evalInterval == evalInterval - 1:\n",
    "                policyRewardSmooth.append(np.mean(policyRewardList[-evalInterval+2:-1]) )\n",
    "                policyRewardMuSmooth.append(np.mean(policyRewardMu[-evalInterval+2:-1]) )\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, k):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateQ(neighborList, 1/pow((m%restartIntervalQ)+1,.4)  )\n",
    "\n",
    "            #perform the grad update\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, k):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateParams(neighborList,  1.0* 1/pow((m%restartIntervalPolicy)+1,.6))\n",
    "                \n",
    "            #print(m%restartIntervalPolicy)\n",
    "            if m > M*0.9: # for the last 10% of running, no restarting\n",
    "                restartIntervalQ = max(int(M*0.5),restartIntervalQ)\n",
    "                restartIntervalPolicy = max(int(M*0.5),restartIntervalPolicy)\n",
    "            #perform a policy evaluation\n",
    "        \n",
    "        return policyRewardSmooth, policyRewardMuSmooth\n",
    "    \n",
    "    def evaluateBenchmarkPolicy(self, M=10000):\n",
    "\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            self.nodeList[i].restart()\n",
    "            self.nodeList[i].setBenchmarkPolicy(self.accessNetwork)\n",
    "            self.nodeList[i].initializeState()\n",
    "            self.nodeList[i].updateAction()\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            neighborList = []\n",
    "            for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                neighborList.append(self.nodeList[j])\n",
    "            self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "\n",
    "        policyRewardList = []\n",
    "\n",
    "        for m in trange(M):\n",
    "            tmpReward = 0.0\n",
    "            for i in range(self.nodeNum): #update state-action\n",
    "                self.nodeList[i].updateState()\n",
    "                self.nodeList[i].updateAction()\n",
    "\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "                tmpReward += self.nodeList[i].reward[-1] # add latest reward\n",
    "            policyRewardList.append(tmpReward/self.nodeNum)\n",
    "\n",
    "        return np.mean(policyRewardList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 1, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 1, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 1]]), array([[1, 1, 0, ..., 0, 0, 0],\n",
      "       [1, 2, 1, ..., 0, 0, 0],\n",
      "       [0, 1, 2, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 2, 1, 0],\n",
      "       [0, 0, 0, ..., 1, 2, 1],\n",
      "       [0, 0, 0, ..., 0, 1, 1]])]\n"
     ]
    }
   ],
   "source": [
    "ddl = 2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# linear network\n",
    "# nodeNum = 5 #number of nodes in the network\n",
    "# maxK = 2 #the size of neighborhood we use in localized learning\n",
    "# arrivalProb = [.3,.9, .9] *2\n",
    "# transmitProb = [ .9,.9]   + [.02, .9, .9 ]*1\n",
    "# networkRLModel = MultiAccessNetworkRL(ddl = 2, graphType = 'line', \\\n",
    "#                                       nodeNum = nodeNum, maxK = maxK ,arrivalProb = arrivalProb,transmitProb = transmitProb)\n",
    "# evalInterval = 500 #evaluate the policy every evalInterval rounds (outer loop)\n",
    "# M = 10000\n",
    "# restartIntervalQ = 500;\n",
    "# restartIntervalPolicy = 500\n",
    "\n",
    "\n",
    "gridW = 6\n",
    "gridH = 6\n",
    "nodeNum = gridW*gridH #number of nodes in the network\n",
    "minK = 0\n",
    "maxK = 1 #the size of neighborhood we use in localized learning\n",
    "arrivalProb = None\n",
    "transmitProb = 'random'\n",
    "networkRLModel = MultiAccessNetworkRL(ddl = 2, graphType = 'grid', \\\n",
    "                                      nodeNum = nodeNum, maxK = maxK ,arrivalProb = arrivalProb,transmitProb = transmitProb,gridW = gridW, gridH = gridH)\n",
    "evalInterval = 500 #evaluate the policy every evalInterval rounds (outer loop)\n",
    "M = 300000\n",
    "restartIntervalQ = 10\n",
    "restartIntervalPolicy = 10\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/10000 [00:00<01:24, 118.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11037514116430513\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11037514116430513\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6563295894652734\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6563295894652734\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1381829513486138\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1381829513486138\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1965823616800535\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1965823616800535\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3687251706609641\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3687251706609641\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8209932298479351\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8209932298479351\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11037514116430513\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09710127579306127\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11037514116430513\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6563295894652734\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09710127579306127\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8379449074988039\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6563295894652734\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1381829513486138\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8379449074988039\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09609840789396307\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1381829513486138\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1965823616800535\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09609840789396307\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9764594650133958\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1965823616800535\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3687251706609641\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9764594650133958\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4686512016477016\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3687251706609641\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8209932298479351\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4686512016477016\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9767610881903371\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8209932298479351\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9767610881903371\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09710127579306127\n",
      "numNodePerAccess =  4.0  transmitProb =  0.604845519745046\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09710127579306127\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8379449074988039\n",
      "numNodePerAccess =  4.0  transmitProb =  0.604845519745046\n",
      "numNodePerAccess =  4.0  transmitProb =  0.7392635793983017\n",
      "numNodePerAccess =  4.0  transmitProb =  0.8379449074988039\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09609840789396307\n",
      "numNodePerAccess =  4.0  transmitProb =  0.7392635793983017\n",
      "numNodePerAccess =  4.0  transmitProb =  0.039187792254320675\n",
      "numNodePerAccess =  4.0  transmitProb =  0.09609840789396307\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9764594650133958\n",
      "numNodePerAccess =  4.0  transmitProb =  0.039187792254320675\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2828069625764096\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9764594650133958\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4686512016477016\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2828069625764096\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1201965612131689\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4686512016477016\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9767610881903371\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1201965612131689\n",
      "numNodePerAccess =  4.0  transmitProb =  0.29614019752214493\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9767610881903371\n",
      "numNodePerAccess =  4.0  transmitProb =  0.29614019752214493\n",
      "numNodePerAccess =  4.0  transmitProb =  0.604845519745046\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11872771895424405\n",
      "numNodePerAccess =  4.0  transmitProb =  0.604845519745046\n",
      "numNodePerAccess =  4.0  transmitProb =  0.7392635793983017\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11872771895424405\n",
      "numNodePerAccess =  4.0  transmitProb =  0.317983179393976\n",
      "numNodePerAccess =  4.0  transmitProb =  0.7392635793983017\n",
      "numNodePerAccess =  4.0  transmitProb =  0.039187792254320675\n",
      "numNodePerAccess =  4.0  transmitProb =  0.317983179393976\n",
      "numNodePerAccess =  4.0  transmitProb =  0.41426299451466997\n",
      "numNodePerAccess =  4.0  transmitProb =  0.039187792254320675\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2828069625764096\n",
      "numNodePerAccess =  4.0  transmitProb =  0.41426299451466997\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06414749634878436\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2828069625764096\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1201965612131689\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06414749634878436\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6924721193700198\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1201965612131689\n",
      "numNodePerAccess =  4.0  transmitProb =  0.29614019752214493\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6924721193700198\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5666014542065752\n",
      "numNodePerAccess =  4.0  transmitProb =  0.29614019752214493\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5666014542065752\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11872771895424405\n",
      "numNodePerAccess =  4.0  transmitProb =  0.11872771895424405\n",
      "numNodePerAccess =  4.0  transmitProb =  0.317983179393976\n",
      "numNodePerAccess =  4.0  transmitProb =  0.317983179393976\n",
      "numNodePerAccess =  4.0  transmitProb =  0.41426299451466997\n",
      "numNodePerAccess =  4.0  transmitProb =  0.41426299451466997\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06414749634878436\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06414749634878436\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6924721193700198\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6924721193700198\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5666014542065752\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5666014542065752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:10<00:00, 141.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark reward is:  0.11381224489795919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchMarkReward = networkRLModel.evaluateBenchmarkPolicy()\n",
    "\n",
    "print('benchmark reward is: ', benchMarkReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [1:03:13<00:00, 105.45it/s]\n",
      "  0%|          | 8/400000 [00:00<1:36:33, 69.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave reward of node  0 =  0.07760570984107384\n",
      "policy of of node  0 =  {(1, 1): array([-1.57095648,  3.57095648]), (1, 0): array([-1.40498682,  3.40498682]), (0, 1): array([-1.69720924,  3.69720924]), (0, 0): array([ 2.36954971, -0.36954971])}\n",
      "ave reward of node  1 =  0.4159914928932764\n",
      "policy of of node  1 =  {(0, 1): array([-1.88317993, -1.7884103 ,  5.67159023]), (1, 0): array([-1.24099652, -1.30358513,  4.54458165]), (0, 0): array([ 4.37801928, -1.14958322, -1.22843606]), (1, 1): array([-1.61684683, -1.59301838,  5.20986521])}\n",
      "ave reward of node  2 =  0.07666464864019949\n",
      "policy of of node  2 =  {(0, 1): array([-1.71926075, -0.7555928 ,  4.47485355]), (1, 0): array([-1.3010464 , -0.33989547,  3.64094187]), (1, 1): array([-1.41255446, -0.89223226,  4.30478672]), (0, 0): array([ 2.53285028, -0.44406825, -0.08878202])}\n",
      "ave reward of node  3 =  0.05259390960660006\n",
      "policy of of node  3 =  {(1, 1): array([-1.53107034,  3.95754832, -0.42647798]), (0, 1): array([-1.66940373,  4.22022293, -0.5508192 ]), (0, 0): array([ 2.54260086, -0.40363878, -0.13896207]), (1, 0): array([-1.39935008,  3.57679665, -0.17744657])}\n",
      "ave reward of node  4 =  0.06331594375326395\n",
      "policy of of node  4 =  {(0, 0): array([ 2.80877019, -0.39433422, -0.41443596]), (0, 1): array([-1.26171447,  4.4646046 , -1.20289013]), (1, 0): array([-0.88417381,  3.63083224, -0.74665843]), (1, 1): array([-0.92937626,  3.83183328, -0.90245703])}\n",
      "ave reward of node  5 =  0.10616369965984815\n",
      "policy of of node  5 =  {(1, 0): array([-1.04276209,  3.85698357, -0.81422148]), (0, 1): array([-1.07150303,  4.07501327, -1.00351025]), (1, 1): array([-1.3839682 ,  4.47403417, -1.09006597]), (0, 0): array([-1.14233491,  1.08923437,  2.05310054])}\n",
      "ave reward of node  6 =  0.028480059514261656\n",
      "policy of of node  6 =  {(0, 0): array([ 2.14055092, -0.14055092]), (0, 1): array([-1.03234187,  3.03234187]), (1, 0): array([-0.57244132,  2.57244132]), (1, 1): array([-0.63121212,  2.63121212])}\n",
      "ave reward of node  7 =  0.0742780555985856\n",
      "policy of of node  7 =  {(1, 1): array([-4.50659406,  6.58459759, -0.07800353]), (1, 0): array([ 8.51568494, -3.53167469, -2.98401024]), (0, 1): array([-3.23187143, -3.49980329,  8.73167472]), (0, 0): array([-3.01162795, -2.61360346,  7.62523141])}\n",
      "ave reward of node  8 =  0.618436714048339\n",
      "policy of of node  8 =  {(1, 1): array([-1.46904285, -1.2921446 , -1.18867941, -0.9077159 ,  6.85758276]), (0, 0): array([ 5.41932514, -0.79995939, -0.84451207, -1.07511251, -0.69974118]), (0, 1): array([-1.06437102, -1.41169503, -1.03982942, -1.48856649,  7.00446196]), (1, 0): array([ 0.10109764, -0.2015309 , -0.55893115, -0.65986849,  3.3192329 ])}\n",
      "ave reward of node  9 =  0.11872579639369404\n",
      "policy of of node  9 =  {(0, 0): array([ 3.94684902, -0.4728234 , -0.54330355, -0.63803918, -0.2926829 ]), (0, 1): array([-1.17281218, -0.72421235, -0.73541723, -1.01169625,  5.64413801]), (1, 1): array([-0.77261036, -0.49916136, -0.44729275, -0.84253302,  4.5615975 ]), (1, 0): array([-1.07546177, -0.41653777, -0.78244643, -0.96675325,  5.24119921])}\n",
      "ave reward of node  10 =  0.20701656020659376\n",
      "policy of of node  10 =  {(0, 1): array([-1.11993275, -1.53467512,  7.61623964, -1.41364967, -1.54798209]), (1, 0): array([-1.25605031, -1.04957974,  6.38697452, -0.62002804, -1.46131644]), (1, 1): array([-0.67782357, -1.88903807,  6.30939904,  0.10223066, -1.84476806]), (0, 0): array([-3.44940966,  6.456843  , -0.24345367, -0.29554656, -0.4684331 ])}\n",
      "ave reward of node  11 =  0.022102591719953525\n",
      "policy of of node  11 =  {(0, 0): array([ 2.47217182, -0.1401662 , -0.06159395, -0.00669048, -0.2637212 ]), (0, 1): array([-1.15112659,  0.94493644, -0.68066488,  2.91423212, -0.02737709]), (1, 0): array([-0.932473  ,  2.36435434, -0.68463539,  0.90357773,  0.34917631]), (1, 1): array([-1.10886181,  1.74908564, -0.75604203,  2.16605334, -0.05023515])}\n",
      "ave reward of node  12 =  0.14040672750344746\n",
      "policy of of node  12 =  {(1, 1): array([-0.98464632, -0.88189183, -1.02022228, -0.27605995,  5.16282038]), (1, 0): array([-0.95495365, -0.89625034, -0.96653502, -0.16110521,  4.97884422]), (0, 0): array([ 3.43323022, -0.49133071, -0.4326722 , -0.37814535, -0.13108196]), (0, 1): array([-0.88347843, -1.10921795, -0.96684259, -0.32462754,  5.2841665 ])}\n",
      "ave reward of node  13 =  0.0763577387258623\n",
      "policy of of node  13 =  {(0, 1): array([ 9.541912  , -3.80482357, -3.73708842]), (1, 1): array([-4.08888116, 10.76726737, -4.67838621]), (1, 0): array([-3.24907756,  9.05070445, -3.80162689]), (0, 0): array([-3.05666261,  7.87259586, -2.81593325])}\n",
      "ave reward of node  14 =  0.006295084483463436\n",
      "policy of of node  14 =  {(1, 0): array([-0.3255223 ,  2.79793868, -0.47241638]), (0, 0): array([ 2.4363228 , -0.16951262, -0.26681018]), (0, 1): array([-0.35345322,  2.91595342, -0.5625002 ]), (1, 1): array([ 1.95075018,  0.05210833, -0.00285851])}\n",
      "ave reward of node  15 =  0.011960106364884008\n",
      "policy of of node  15 =  {(0, 1): array([-0.62567447,  1.73912208, -0.66487896, -0.69010227,  2.24153361]), (1, 0): array([-0.35423352,  3.0325106 , -0.53379378, -0.55893271,  0.4144494 ]), (0, 0): array([ 1.58939304,  0.28881985,  0.31499108, -0.28621795,  0.09301398]), (1, 1): array([ 1.69308128,  0.23089367, -0.04309758,  0.02889833,  0.09022429])}\n",
      "ave reward of node  16 =  0.0015412241217662409\n",
      "policy of of node  16 =  {(0, 0): array([ 2.05011146, -0.11194061, -0.01320028,  0.08884538, -0.01381595]), (0, 1): array([ 0.7402661 , -0.17850773,  0.45146695,  0.48874944,  0.49802523]), (1, 0): array([ 0.77039308, -0.09884429,  0.49092484,  0.30259524,  0.53493113]), (1, 1): array([1.93210818, 0.00784559, 0.00486168, 0.00435272, 0.05083184])}\n",
      "ave reward of node  17 =  0.21923067942592586\n",
      "policy of of node  17 =  {(1, 1): array([-2.18826616, -1.93428026,  9.81783706, -1.55707911, -2.13821153]), (0, 1): array([-2.3938382 , -1.48157057,  9.75311581, -1.76731912, -2.11038791]), (1, 0): array([-1.74321997, -1.72410098,  8.61106504, -1.05754294, -2.08620115]), (0, 0): array([-2.88550112, -1.82062378,  8.57538508, -0.53340374, -1.33585644])}\n",
      "ave reward of node  18 =  0.3641946839254318\n",
      "policy of of node  18 =  {(1, 1): array([-1.7398789 , -1.06892417, -1.43511496, -0.04442936,  6.28834739]), (1, 0): array([-1.29571799, -0.84930066, -0.69738536, -0.22730099,  5.069705  ]), (0, 1): array([-1.50947715, -0.99962452, -1.3719653 , -0.44037137,  6.32143834]), (0, 0): array([ 1.6108649 ,  0.0227812 , -0.437922  ,  0.15618711,  0.64808879])}\n",
      "ave reward of node  19 =  0.2450827308169576\n",
      "policy of of node  19 =  {(1, 1): array([-2.58762854, 12.18228988, -2.6767427 , -1.79119764, -3.126721  ]), (0, 1): array([-2.29001648, 11.65691992, -2.28422874, -1.67956619, -3.40310851]), (1, 0): array([-1.43986928, 10.20596718, -2.36235417, -1.91203077, -2.49171296]), (0, 0): array([-3.0477101 , -1.84799509, -1.448878  , 10.13041717, -1.78583399])}\n",
      "ave reward of node  20 =  0.8075964164958999\n",
      "policy of of node  20 =  {(1, 1): array([-2.01767824, -1.61486859,  5.63254683]), (1, 0): array([ 2.13108059,  0.38906358, -0.52014417]), (0, 1): array([-2.46768878, -2.71952641,  7.18721519]), (0, 0): array([ 5.09102116, -1.57865988, -1.51236128])}\n",
      "ave reward of node  21 =  0.07634912012841243\n",
      "policy of of node  21 =  {(0, 1): array([-3.61945069,  8.62554851, -3.00609782]), (1, 1): array([-3.7632946 ,  9.28157596, -3.51828136]), (1, 0): array([ 8.02624047, -2.90143373, -3.12480674]), (0, 0): array([-3.27477483,  7.8356691 , -2.56089426])}\n",
      "ave reward of node  22 =  0.0888518911107903\n",
      "policy of of node  22 =  {(0, 1): array([-1.16146323, -1.30337249, -0.45461414, -1.18897491,  6.10842476]), (1, 0): array([-1.00907977, -1.00145433,  0.16223437, -0.85260856,  4.70090828]), (0, 0): array([ 4.23551612, -0.76447052, -0.38813556, -0.62516652, -0.45774352]), (1, 1): array([-0.95999558, -0.9132115 , -0.17220924, -0.85011781,  4.89553413])}\n",
      "ave reward of node  23 =  0.4796528123166483\n",
      "policy of of node  23 =  {(0, 1): array([-1.37525442,  7.06920987, -1.58489626, -0.8874202 , -1.22163899]), (1, 1): array([-1.20776076,  6.44267554, -1.13659662, -0.74533964, -1.35297852]), (1, 0): array([-0.79793225,  5.2297483 , -0.93555937, -0.50135472, -0.99490195]), (0, 0): array([ 5.28971958, -0.87317404, -0.85671902, -0.91683087, -0.64299565])}\n",
      "ave reward of node  24 =  0.0593153264644888\n",
      "policy of of node  24 =  {(0, 0): array([ 2.52795543, -0.23351618, -0.03647044,  0.22468647, -0.48265527]), (0, 1): array([-0.9660944 ,  0.04106472,  4.00569169, -0.34491963, -0.73574238]), (1, 1): array([ 1.27039909,  0.40888731,  0.38346747,  0.03055043, -0.0933043 ]), (1, 0): array([-0.7493218 ,  0.13450687,  3.75070198, -0.42561574, -0.7102713 ])}\n",
      "ave reward of node  25 =  0.05602796757565137\n",
      "policy of of node  25 =  {(1, 1): array([-1.23190837, -0.65157455, -1.03420382,  5.53288132, -0.61519458]), (0, 1): array([-1.2514821 , -0.53933206, -1.05006994,  5.52018452, -0.67930043]), (1, 0): array([-1.40435748,  1.19345072, -0.81366648,  3.76494186, -0.74036862]), (0, 0): array([ 3.77927061, -0.47098749, -0.2888689 , -0.53134155, -0.48807267])}\n",
      "ave reward of node  26 =  0.012716992043024766\n",
      "policy of of node  26 =  {(0, 0): array([ 2.62421173, -0.16300171, -0.02719937, -0.22195109, -0.21205956]), (0, 1): array([-0.9582704 ,  0.70391779, -0.72296994,  2.89339924,  0.08392331]), (1, 0): array([-0.49237136,  0.00849528, -0.57813561,  3.20268408, -0.1406724 ]), (1, 1): array([ 1.38260482, -0.05199153,  0.06476317,  0.43632826,  0.16829528])}\n",
      "ave reward of node  27 =  0.9001836547374021\n",
      "policy of of node  27 =  {(0, 1): array([-2.42898974, -2.40730029,  6.83629003]), (1, 1): array([-1.32771127, -1.40346359,  4.73117486]), (1, 0): array([ 0.51861914, -0.3998057 ,  1.88118656]), (0, 0): array([ 4.75271576, -1.38519719, -1.36751857])}\n",
      "ave reward of node  28 =  0.06894155111865617\n",
      "policy of of node  28 =  {(0, 0): array([ 3.18556519, -0.65840767, -0.52715751]), (0, 1): array([-1.47062291, -1.14005899,  4.6106819 ]), (1, 0): array([-1.13940665, -0.61334394,  3.75275059]), (1, 1): array([-1.36689291, -0.58638844,  3.95328134])}\n",
      "ave reward of node  29 =  0.09581307140786591\n",
      "policy of of node  29 =  {(0, 0): array([ 4.01609636, -0.26020765, -0.66961767, -0.50662856, -0.57964247]), (0, 1): array([-1.11212597, -0.99732271,  5.98197817, -0.92179658, -0.9507329 ]), (1, 0): array([-0.87002708, -0.79335331,  4.84447486, -0.81353928, -0.36755518]), (1, 1): array([-0.65141029, -0.77794441,  4.72582553, -0.71904837, -0.57742246])}\n",
      "ave reward of node  30 =  0.046667885164390806\n",
      "policy of of node  30 =  {(0, 0): array([ 3.18070711, -0.38450943, -0.43021907, -0.34448886, -0.02148973]), (0, 1): array([-0.8096207 ,  5.32600021, -0.75167181, -0.56559036, -1.19911734]), (1, 1): array([-0.21908921,  3.18961985, -0.04195547, -0.39780104, -0.53077414]), (1, 0): array([-0.69777925,  3.80177922,  0.08776218, -0.263518  , -0.92824416])}\n",
      "ave reward of node  31 =  0.1282148853202243\n",
      "policy of of node  31 =  {(1, 1): array([-0.95894755, -1.14525955,  6.30322587, -1.21071916, -0.98829961]), (0, 1): array([-0.93609879, -1.28171773,  6.24295921, -1.18785722, -0.83728547]), (1, 0): array([-0.71976259, -0.89308641,  5.41764625, -1.03364913, -0.77114812]), (0, 0): array([-1.9323126 ,  3.09091389,  0.98017878, -0.06378975, -0.07499032])}\n",
      "ave reward of node  32 =  0.17668037601133488\n",
      "policy of of node  32 =  {(0, 0): array([ 3.96161696, -0.58947385, -0.18766825, -0.86560363, -0.31887124]), (0, 1): array([-1.13629051, -0.79691567,  5.29492468, -0.84523144, -0.51648705]), (1, 1): array([-0.90706482, -0.67673415,  4.67724784, -0.31978107, -0.77366779]), (1, 0): array([-0.86895168, -0.66377832,  4.89598065, -0.78237537, -0.58087528])}\n",
      "ave reward of node  33 =  0.08979090867279682\n",
      "policy of of node  33 =  {(1, 1): array([-1.52682382,  1.38310843, -1.12907261, -0.37478604,  3.64757404]), (0, 0): array([ 3.82550539, -0.31678585, -0.42444351, -0.40675343, -0.6775226 ]), (0, 1): array([-1.28462024,  4.95159698, -1.12888219, -1.05717993,  0.51908538]), (1, 0): array([-1.44881944,  1.64050471, -1.15196349, -0.67036274,  3.63064096])}\n",
      "ave reward of node  34 =  1.666700795154937e-05\n",
      "policy of of node  34 =  {(0, 0): array([ 2.01072144,  0.07581604, -0.08653748]), (0, 1): array([0.95820633, 0.21506358, 0.8267301 ]), (1, 0): array([1.06988631, 0.12259122, 0.80752247]), (1, 1): array([ 1.98310543, -0.00249085,  0.01938542])}\n",
      "ave reward of node  35 =  0.10652122795095285\n",
      "policy of of node  35 =  {(1, 1): array([-1.43108422,  4.81437817, -1.38329396]), (1, 0): array([-1.08850508,  4.35486678, -1.26636169]), (0, 1): array([-1.44434542,  4.9565374 , -1.51219198]), (0, 0): array([ 3.38996625, -0.70855904, -0.68140722])}\n",
      "ave reward of node  36 =  0.07778521282683358\n",
      "policy of of node  36 =  {(1, 0): array([-0.76096638, -0.6179937 ,  4.98334596, -0.8659622 , -0.73842368]), (0, 1): array([-1.09901085, -0.80669733,  6.00288419, -1.1657887 , -0.93138731]), (1, 1): array([-0.93261413, -0.98689496,  5.6468207 , -0.97759715, -0.74971447]), (0, 0): array([ 3.39657275, -0.46074411, -0.33159059, -0.18985813, -0.41437992])}\n",
      "ave reward of node  37 =  0.12881779015012312\n",
      "policy of of node  37 =  {(1, 0): array([-0.85218362,  4.98258866, -0.93227204, -0.57346774, -0.62466526]), (0, 0): array([ 3.7546521 , -0.53598605, -0.32221154, -0.37192254, -0.52453198]), (0, 1): array([-1.18330869,  6.03452484, -1.29995203, -0.88332709, -0.66793703]), (1, 1): array([-1.00617941,  5.5937148 , -1.27777071, -0.58473823, -0.72502645])}\n",
      "ave reward of node  38 =  0.21491514163840666\n",
      "policy of of node  38 =  {(1, 1): array([-2.83301416, -2.99009406, 13.51622058, -2.76100672, -2.93210563]), (0, 1): array([-2.6687259 , 12.73756923, -3.22875645, -2.36997375, -2.47011313]), (1, 0): array([-2.3763015 , -2.19887773, 11.26686919, -2.22752117, -2.46416879]), (0, 0): array([-2.41643433, -1.84669004, 10.44844505, -2.16197806, -2.02334263])}\n",
      "ave reward of node  39 =  0.34890785490179954\n",
      "policy of of node  39 =  {(0, 1): array([-1.0647267 , -1.16309323, -1.27068328, -1.37142352,  6.86992673]), (1, 1): array([-1.0351579 , -1.04494864, -0.96889883, -1.05251261,  6.10151798]), (1, 0): array([-0.78418889, -0.87145428, -0.73892303, -1.03027932,  5.42484552]), (0, 0): array([ 4.69331751, -0.698466  , -0.8270189 , -0.46964478, -0.69818783])}\n",
      "ave reward of node  40 =  0.06697795944321645\n",
      "policy of of node  40 =  {(0, 0): array([ 3.92442578, -0.52770972, -0.51243873, -0.24615489, -0.63812244]), (0, 1): array([-1.44572048, -0.81706278,  5.10305276, -0.33786221, -0.50240729]), (1, 1): array([-0.86548896, -0.50799657,  3.80463505, -0.33032303, -0.10082649]), (1, 0): array([-1.22560413, -0.8111918 ,  4.8690989 , -0.48789601, -0.34440696])}\n",
      "ave reward of node  41 =  0.1599217948998721\n",
      "policy of of node  41 =  {(0, 0): array([ 3.38427009, -0.51162183, -0.87264826]), (0, 1): array([-1.87013315, -1.04161215,  4.9117453 ]), (1, 1): array([-1.53546054, -0.51249315,  4.04795369]), (1, 0): array([-1.42507665, -0.61767989,  4.04275654])}\n",
      "ave reward of node  42 =  0.11996061452145614\n",
      "policy of of node  42 =  {(1, 1): array([-3.5904563,  5.5904563]), (1, 0): array([-3.76351572,  5.76351572]), (0, 1): array([-4.01766981,  6.01766981]), (0, 0): array([-3.71632783,  5.71632783])}\n",
      "ave reward of node  43 =  2.2933182577311946e-06\n",
      "policy of of node  43 =  {(0, 0): array([ 1.98329505,  0.06384316, -0.04713821]), (0, 1): array([ 0.28000555, -0.18022397,  1.90021841]), (1, 0): array([ 0.44203423, -0.04777117,  1.60573694]), (1, 1): array([ 1.94990604e+00, -1.41474428e-03,  5.15087040e-02])}\n",
      "ave reward of node  44 =  0.24217160557513323\n",
      "policy of of node  44 =  {(1, 1): array([-1.58403721,  4.95751943, -1.37348222]), (1, 0): array([-1.42559679,  4.41657856, -0.99098177]), (0, 1): array([-1.70043899,  4.92641932, -1.22598032]), (0, 0): array([ 3.40904845, -0.82331093, -0.58573751])}\n",
      "ave reward of node  45 =  0.2929671456702175\n",
      "policy of of node  45 =  {(0, 1): array([-1.54538887,  5.26983592, -1.72444704]), (1, 0): array([-1.33295134,  4.61045638, -1.27750504]), (1, 1): array([-1.52571827,  4.96730784, -1.44158956]), (0, 0): array([ 3.76677607, -0.79151148, -0.97526459])}\n",
      "ave reward of node  46 =  0.0334393071474829\n",
      "policy of of node  46 =  {(0, 0): array([ 2.84922811, -0.46358719, -0.38564092]), (0, 1): array([-1.13981112, -1.07232421,  4.21213533]), (1, 1): array([-0.09679701, -0.15957777,  2.25637478]), (1, 0): array([-0.85921757, -0.22773211,  3.08694969])}\n",
      "ave reward of node  47 =  0.008043575336487955\n",
      "policy of of node  47 =  {(0, 0): array([ 1.94252762, -0.05240122,  0.10987359]), (0, 1): array([-1.24929333,  0.3218625 ,  2.92743083]), (1, 0): array([-1.07124204,  0.97255328,  2.09868876]), (1, 1): array([1.12698749, 0.14949629, 0.72351623])}\n",
      "ave reward of node  48 =  0.06725139710466253\n",
      "policy of of node  48 =  {(0, 0): array([ 2.7559943, -0.7559943]), (0, 1): array([-1.85180214,  3.85180214]), (1, 0): array([-1.43501908,  3.43501908]), (1, 1): array([-1.00898331,  3.00898331])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [4:12:04<00:00, 26.45it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave reward of node  0 =  0.27115395459841984\n",
      "policy of of node  0 =  {(1, 1): array([-1.93150723,  3.93150723]), (1, 0): array([-1.89788918,  3.89788918]), (0, 1): array([-2.26740373,  4.26740373]), (0, 0): array([ 2.76354259, -0.76354259])}\n",
      "ave reward of node  1 =  0.4470853823392338\n",
      "policy of of node  1 =  {(1, 1): array([-1.19268846, -2.01442319,  5.20711165]), (1, 0): array([-0.62273167, -1.61976899,  4.24250066]), (0, 1): array([-1.56065626, -1.9904096 ,  5.55106585]), (0, 0): array([-0.29406941,  0.82084516,  1.47322425])}\n",
      "ave reward of node  2 =  0.3731811795844744\n",
      "policy of of node  2 =  {(1, 1): array([-1.16469134, -1.66382932,  4.82852066]), (1, 0): array([-0.73514679, -1.66254531,  4.3976921 ]), (0, 1): array([-1.27966311, -1.89745165,  5.17711475]), (0, 0): array([0.4223223 , 0.79406595, 0.78361175])}\n",
      "ave reward of node  3 =  0.1699975449038472\n",
      "policy of of node  3 =  {(0, 0): array([1.36063827, 0.37353528, 0.26582645]), (0, 1): array([ 1.06023034, -2.18715124,  3.1269209 ]), (1, 1): array([ 4.06758673, -1.68371954, -0.38386719]), (1, 0): array([ 3.92883637, -1.44461053, -0.48422585])}\n",
      "ave reward of node  4 =  0.36394130316176826\n",
      "policy of of node  4 =  {(0, 1): array([-1.685578  ,  5.29106813, -1.60549013]), (1, 1): array([-0.41783559,  2.9849111 , -0.56707551]), (1, 0): array([-0.64437939,  3.4513438 , -0.80696441]), (0, 0): array([0.50946305, 0.60657574, 0.8839612 ])}\n",
      "ave reward of node  5 =  0.05827998832113533\n",
      "policy of of node  5 =  {(1, 0): array([ 1.58889919,  2.00010722, -1.58900641]), (0, 0): array([ 2.79190603, -0.52025786, -0.27164817]), (0, 1): array([-0.20218924,  4.00131754, -1.7991283 ]), (1, 1): array([ 2.85268098,  0.84705843, -1.69973941])}\n",
      "ave reward of node  6 =  0.13255523869961836\n",
      "policy of of node  6 =  {(1, 1): array([-1.4228179,  3.4228179]), (1, 0): array([-1.4799779,  3.4799779]), (0, 1): array([-1.6743882,  3.6743882]), (0, 0): array([0.83960328, 1.16039672])}\n",
      "ave reward of node  7 =  0.0952215011059719\n",
      "policy of of node  7 =  {(0, 1): array([-0.18342531, -1.93323152,  4.11665683]), (1, 1): array([-1.4656373 , -1.98873561,  5.4543729 ]), (1, 0): array([ 2.44718946, -0.98743511,  0.54024565]), (0, 0): array([ 2.21509059, -0.06211412, -0.15297647])}\n",
      "ave reward of node  8 =  0.006906355321194993\n",
      "policy of of node  8 =  {(0, 1): array([ 3.93966367, -0.733564  , -0.50699851, -0.39538206, -0.30371911]), (1, 1): array([ 6.7722185 , -1.87401297, -1.0461246 , -1.21793562, -0.63414531]), (0, 0): array([ 2.06492993,  0.01329212, -0.03319956, -0.02545336, -0.01956913]), (1, 0): array([ 3.78867884, -0.74436518, -0.41592697, -0.26148771, -0.36689897])}\n",
      "ave reward of node  9 =  0.0006337589841106648\n",
      "policy of of node  9 =  {(0, 1): array([ 4.15101787, -1.65334213, -1.13343735,  0.59899489,  0.03676671]), (1, 0): array([ 4.56294066, -1.54826368, -0.97530853, -0.22689429,  0.18752583]), (1, 1): array([ 4.46296971, -1.26423826, -0.83368439, -0.37081523,  0.00576817]), (0, 0): array([-0.37538901,  0.02899869, -0.15510615, -0.23713417,  2.73863064])}\n",
      "ave reward of node  10 =  0.012276361505971573\n",
      "policy of of node  10 =  {(1, 0): array([ 4.43573181, -0.97207932, -0.59830171, -0.47706775, -0.38828303]), (0, 0): array([ 2.92236899, -0.2224746 , -0.16011414, -0.34034207, -0.19943818]), (0, 1): array([ 4.46325778, -1.05217055, -0.83291595, -0.39611817, -0.18205311]), (1, 1): array([ 5.03777436, -1.69933618, -1.05226087, -0.44501621,  0.1588389 ])}\n",
      "ave reward of node  11 =  0.005958565485850221\n",
      "policy of of node  11 =  {(1, 1): array([ 3.72081077, -1.43087329, -0.34770609, -0.10004141,  0.15781001]), (1, 0): array([ 3.75738104, -1.55214076, -0.0682965 , -0.0380522 , -0.09889158]), (0, 0): array([ 1.5710691 ,  0.26067742,  0.57013934, -0.41805157,  0.01616571]), (0, 1): array([ 3.31361688, -1.66809405,  0.17638836,  0.043578  ,  0.13451081])}\n",
      "ave reward of node  12 =  0.07089816488895892\n",
      "policy of of node  12 =  {(0, 0): array([ 3.6223034 , -0.42479575, -0.46528857, -0.1466528 , -0.58556628]), (0, 1): array([ 5.01489838, -0.53898973, -1.26915323, -0.2925878 , -0.91416763]), (1, 0): array([ 4.48845367,  0.39839449, -1.35544334, -0.48901768, -1.04238714]), (1, 1): array([ 0.84113144, -0.37893251, -1.94214624,  5.02451866, -1.54457135])}\n",
      "ave reward of node  13 =  0.14614307030815937\n",
      "policy of of node  13 =  {(1, 1): array([-1.21261125, -1.65496825,  4.8675795 ]), (1, 0): array([ 0.22649956, -0.72327902,  2.49677945]), (0, 1): array([-0.31238654, -1.49501164,  3.80739817]), (0, 0): array([ 2.31913445, -0.10740022, -0.21173423])}\n",
      "ave reward of node  14 =  8.391720359130025e-23\n",
      "policy of of node  14 =  {(0, 1): array([ 2.93238818, -0.61349259, -0.31889559]), (1, 0): array([ 2.89611109, -0.71422844, -0.18188265]), (0, 0): array([ 1.23463863, -0.06678746,  0.83214883]), (1, 1): array([ 2.10509112, -0.07324897, -0.03184216])}\n",
      "ave reward of node  15 =  0.012481098332483903\n",
      "policy of of node  15 =  {(0, 0): array([-0.16878439,  1.28188055,  0.06317872, -0.42651691,  1.25024203]), (0, 1): array([ 2.72454101, -0.80817616,  0.39545508, -0.15868919, -0.15313075]), (1, 1): array([ 1.93253495e+00, -5.01544761e-02,  2.78854051e-02,  8.82374818e-02,\n",
      "        1.49664309e-03]), (1, 0): array([ 2.94731021, -0.89371233,  0.08069838, -0.14984744,  0.01555118])}\n",
      "ave reward of node  16 =  0.002733270861597986\n",
      "policy of of node  16 =  {(0, 0): array([0.16945763, 0.60808156, 0.36192142, 0.43050176, 0.43003763]), (0, 1): array([1.99212526e+00, 1.23650046e-03, 2.21377260e-03, 2.21154122e-03,\n",
      "       2.21292118e-03]), (1, 0): array([1.98711548e+00, 9.67089152e-03, 2.83590826e-03, 1.83626485e-04,\n",
      "       1.94089408e-04]), (1, 1): array([2., 0., 0., 0., 0.])}\n",
      "ave reward of node  17 =  0.1275740067137287\n",
      "policy of of node  17 =  {(1, 0): array([1.85282298, 0.02618629, 0.03148831, 0.05957461, 0.02992782]), (0, 1): array([1.54881231, 0.11302156, 0.11529654, 0.11728487, 0.10558472]), (1, 1): array([ 0.61365111,  0.62892067, -0.05895809,  0.45028487,  0.36610144]), (0, 0): array([ 1.98981036e+00,  4.76856763e-03,  1.25066614e-03, -9.43002616e-04,\n",
      "        5.11341107e-03])}\n",
      "ave reward of node  18 =  0.24918736704423852\n",
      "policy of of node  18 =  {(1, 1): array([-0.34588046,  1.88134881, -0.52135392,  0.14300117,  0.8428844 ]), (1, 0): array([1.69410361, 0.10969058, 0.01655206, 0.07023733, 0.10941641]), (0, 1): array([ 0.45897887,  0.66581901, -0.1442244 ,  0.4121197 ,  0.60730681]), (0, 0): array([1.90225433, 0.02348163, 0.00649619, 0.0548622 , 0.01290566])}\n",
      "ave reward of node  19 =  2.40371783169765e-06\n",
      "policy of of node  19 =  {(1, 1): array([ 6.26787913, -0.9495339 , -0.65599352, -0.78155755, -1.88079415]), (1, 0): array([ 4.62920811, -0.5204337 , -0.43572332, -0.27375509, -1.399296  ]), (0, 1): array([ 4.70940614, -0.47782577, -0.37175607, -0.50469204, -1.35513225]), (0, 0): array([ 2.88610174, -0.25295894, -0.31059158, -0.12369778, -0.19885343])}\n",
      "ave reward of node  20 =  0.7933482282548198\n",
      "policy of of node  20 =  {(0, 1): array([-2.00959473, -2.04094771,  6.05054244]), (1, 1): array([-1.43323315, -1.65542445,  5.0886576 ]), (1, 0): array([1.50012375, 0.00970971, 0.49016654]), (0, 0): array([1.45113679, 0.18028621, 0.36857699])}\n",
      "ave reward of node  21 =  0.12902622302268932\n",
      "policy of of node  21 =  {(1, 1): array([-1.65019242,  3.69187534, -0.04168292]), (0, 1): array([-1.04910135,  2.30619668,  0.74290467]), (1, 0): array([-0.59195929,  1.80561197,  0.78634731]), (0, 0): array([1.25857464, 0.32588166, 0.4155437 ])}\n",
      "ave reward of node  22 =  0.08403619036357862\n",
      "policy of of node  22 =  {(1, 1): array([ 2.42824277, -0.70466976,  0.27079541, -0.14430652,  0.1499381 ]), (1, 0): array([ 2.43560988, -0.75858309,  0.38030345, -0.27402576,  0.21669552]), (0, 1): array([ 2.25791194, -1.16245276,  0.74900603, -0.34852655,  0.50406133]), (0, 0): array([ 1.73219764, -0.02931926,  0.22198872, -0.26227312,  0.33740601])}\n",
      "ave reward of node  23 =  0.2536418116230082\n",
      "policy of of node  23 =  {(0, 1): array([1.15323725, 0.25316452, 0.21829878, 0.23768578, 0.13761367]), (1, 1): array([0.53304497, 0.27156107, 0.33411751, 0.59834031, 0.26293613]), (1, 0): array([1.78811625, 0.05354933, 0.04561813, 0.07175316, 0.04096313]), (0, 0): array([1.91470189, 0.02107857, 0.01836907, 0.02499747, 0.02085301])}\n",
      "ave reward of node  24 =  0.010353305269516793\n",
      "policy of of node  24 =  {(0, 1): array([1.94511837, 0.02089433, 0.01211389, 0.00760678, 0.01426663]), (1, 0): array([ 1.99302604e+00,  8.20951584e-04, -7.48879300e-03, -1.91964516e-03,\n",
      "        1.55614496e-02]), (0, 0): array([1.18331601, 0.18342667, 0.28111046, 0.16210946, 0.1900374 ]), (1, 1): array([ 2.00293187e+00,  4.63777281e-05, -3.07100170e-03,  4.63777281e-05,\n",
      "        4.63777281e-05])}\n",
      "ave reward of node  25 =  0.21908422861146093\n",
      "policy of of node  25 =  {(0, 1): array([0.55388291, 0.27719592, 0.29923779, 0.56146576, 0.30821763]), (0, 0): array([1.57403272, 0.12051836, 0.1007614 , 0.15642874, 0.04825877]), (1, 0): array([1.51454352, 0.10084589, 0.01334314, 0.19032423, 0.18094322]), (1, 1): array([0.75159756, 0.04757976, 0.17409326, 0.39847063, 0.62825879])}\n",
      "ave reward of node  26 =  4.859902170553536e-07\n",
      "policy of of node  26 =  {(0, 1): array([ 5.08887495, -0.18613285, -1.11053347, -0.40508492, -1.38712371]), (1, 0): array([ 5.09357126, -0.29600667, -1.09215506, -0.23037969, -1.47502983]), (0, 0): array([ 4.43090274, -1.0333961 , -0.45575401, -0.62098238, -0.32077025]), (1, 1): array([ 3.65448191, -0.11629866, -0.64029162, -0.10983614, -0.78805549])}\n",
      "ave reward of node  27 =  0.9248768073325417\n",
      "policy of of node  27 =  {(0, 1): array([-2.10906501, -2.34868067,  6.45774568]), (1, 0): array([ 1.91517611, -0.11959646,  0.20442035]), (1, 1): array([-1.01921261, -1.18977083,  4.20898344]), (0, 0): array([1.31968317, 0.56699143, 0.1133254 ])}\n",
      "ave reward of node  28 =  8.931760696869774e-08\n",
      "policy of of node  28 =  {(0, 1): array([ 3.71901512,  0.14798313, -1.86699825]), (1, 0): array([ 3.93981652, -0.28697001, -1.65284651]), (0, 0): array([ 2.93305383, -0.51391574, -0.41913809]), (1, 1): array([ 3.67009063, -0.07101725, -1.59907339])}\n",
      "ave reward of node  29 =  0.013266021367605949\n",
      "policy of of node  29 =  {(0, 1): array([ 4.39863219, -0.34017693, -0.25697708, -1.48905508, -0.3124231 ]), (1, 0): array([ 4.10921903, -0.41795021, -0.15707184, -1.45879557, -0.07540141]), (1, 1): array([ 3.98192465, -0.16574128, -0.32267058, -1.26240597, -0.23110682]), (0, 0): array([ 3.38602788, -0.14577232, -0.18882779, -0.38744644, -0.66398133])}\n",
      "ave reward of node  30 =  0.09813657294586098\n",
      "policy of of node  30 =  {(1, 0): array([1.32705931, 0.13925116, 0.20054749, 0.18487005, 0.14827199]), (0, 0): array([0.11613447, 0.64712953, 0.3215803 , 0.39144689, 0.52370881]), (0, 1): array([1.01814775, 0.3050334 , 0.17802709, 0.27707294, 0.22171883]), (1, 1): array([1.86658237, 0.04179565, 0.03336845, 0.03156248, 0.02669105])}\n",
      "ave reward of node  31 =  0.17436649642815394\n",
      "policy of of node  31 =  {(1, 1): array([0.59202567, 0.21700388, 0.60302103, 0.48667   , 0.10127942]), (1, 0): array([1.57973465, 0.10287888, 0.09642792, 0.10995438, 0.11100416]), (0, 1): array([1.32654914, 0.17578233, 0.23024637, 0.16621423, 0.10120792]), (0, 0): array([ 1.90166867,  0.05309871, -0.00787707,  0.03669068,  0.01641901])}\n",
      "ave reward of node  32 =  0.15603503666834498\n",
      "policy of of node  32 =  {(0, 0): array([0.51952751, 0.45755766, 0.29315253, 0.29148297, 0.43827933]), (0, 1): array([0.39658319, 0.63961481, 0.48767393, 0.15486492, 0.32126316]), (1, 1): array([1.12170474, 0.29147899, 0.31592886, 0.13174082, 0.1391466 ]), (1, 0): array([1.18324065, 0.28053353, 0.23584975, 0.1358754 , 0.16450068])}\n",
      "ave reward of node  33 =  0.00015895777286270302\n",
      "policy of of node  33 =  {(1, 0): array([ 4.95779889, -0.15368343, -2.15565165, -0.14129542, -0.50716839]), (0, 1): array([ 4.97737427, -0.2538529 , -2.12417372, -0.04888956, -0.5504581 ]), (1, 1): array([ 5.1406129 , -0.14314565, -2.07856277, -0.27256583, -0.64633865]), (0, 0): array([ 3.19753288e+00,  1.01633576e-03, -5.00982427e-01, -3.84330232e-01,\n",
      "       -3.13236561e-01])}\n",
      "ave reward of node  34 =  2.7299442699084446e-36\n",
      "policy of of node  34 =  {(0, 0): array([ 3.35998272, -0.80762109, -0.55236163]), (0, 1): array([ 2.98956781, -0.98062024, -0.00894757]), (1, 0): array([ 3.09939426, -0.97190194, -0.12749231]), (1, 1): array([ 2.02467856, -0.01374148, -0.01093708])}\n",
      "ave reward of node  35 =  0.41709577473594023\n",
      "policy of of node  35 =  {(0, 1): array([-1.60417676,  5.42466476, -1.820488  ]), (1, 0): array([-0.96358291,  4.36524668, -1.40166377]), (0, 0): array([1.85851999, 0.03882104, 0.10265897]), (1, 1): array([-1.10618251,  4.67124939, -1.56506688])}\n",
      "ave reward of node  36 =  0.007447262267862012\n",
      "policy of of node  36 =  {(0, 1): array([ 4.87426155, -1.18702004, -0.46307702, -0.88535426, -0.33881023]), (1, 1): array([ 4.96309232, -1.37385415, -0.11138071, -1.01083511, -0.46702235]), (1, 0): array([ 4.27357671, -1.12658724, -0.21532161, -1.02146781,  0.08979995]), (0, 0): array([ 3.62078159, -0.55941734, -0.28369027, -0.29080141, -0.48687257])}\n",
      "ave reward of node  37 =  0.022839159347370644\n",
      "policy of of node  37 =  {(0, 0): array([ 2.0375782 , -0.0902945 ,  0.13266368, -0.2670059 ,  0.18705852]), (0, 1): array([ 3.19087293,  0.26340436, -0.26977961,  0.1902458 , -1.37474347]), (1, 1): array([ 3.61318349,  0.0227184 ,  0.1139172 , -0.14813372, -1.60168536]), (1, 0): array([ 3.86260627,  0.23226823, -0.43492835, -0.38003994, -1.27990621])}\n",
      "ave reward of node  38 =  8.33428667794737e-08\n",
      "policy of of node  38 =  {(1, 1): array([ 5.02436562, -0.72940496, -0.2347163 , -1.79497048, -0.26527388]), (0, 1): array([ 3.11308233, -0.10015958, -0.22830761, -0.5741081 , -0.21050704]), (1, 0): array([ 3.12229624, -0.13895426, -0.14554461, -0.58403484, -0.25376253]), (0, 0): array([ 2.03541596, -0.0123611 , -0.00415152, -0.03191895,  0.01301559])}\n",
      "ave reward of node  39 =  0.1252451497391252\n",
      "policy of of node  39 =  {(0, 1): array([ 1.95485235,  0.67932647,  1.17377607, -0.3747805 , -1.43317439]), (1, 1): array([ 1.11986662,  2.92793122,  0.8467388 , -0.98708262, -1.90745402]), (1, 0): array([ 1.90875581,  0.52895545,  0.79901034, -0.1227236 , -1.11399799]), (0, 0): array([1.1789437 , 0.33951379, 0.12646146, 0.07782462, 0.27725642])}\n",
      "ave reward of node  40 =  0.004905230361737813\n",
      "policy of of node  40 =  {(1, 1): array([ 4.12121977,  0.12259018, -0.86500053, -0.29371789, -1.08509153]), (1, 0): array([ 4.43111986,  0.41591262, -1.12537698, -0.28939149, -1.43226401]), (0, 0): array([ 3.78919559, -0.4672614 , -0.14380729, -1.31712474,  0.13899785]), (0, 1): array([ 4.4769827 ,  0.07401185, -1.10554689,  0.07850144, -1.5239491 ])}\n",
      "ave reward of node  41 =  0.1784115139872422\n",
      "policy of of node  41 =  {(1, 1): array([-0.92468298,  3.98102888, -1.0563459 ]), (1, 0): array([-1.16398703,  4.32626996, -1.16228294]), (0, 0): array([ 2.87613361, -0.1474134 , -0.72872021]), (0, 1): array([-1.44629866,  4.64424279, -1.19794413])}\n",
      "ave reward of node  42 =  0.11591033340146485\n",
      "policy of of node  42 =  {(0, 0): array([1.93729166, 0.06270834]), (0, 1): array([-1.14945261,  3.14945261]), (1, 0): array([-1.1673393,  3.1673393]), (1, 1): array([-1.64714203,  3.64714203])}\n",
      "ave reward of node  43 =  0.00018950239085308018\n",
      "policy of of node  43 =  {(0, 0): array([ 4.16436342, -0.78852934, -1.37583408]), (0, 1): array([ 2.79690884, -0.84793923,  0.05103039]), (1, 0): array([ 3.03862055, -0.90793821, -0.13068234]), (1, 1): array([ 2.16199293, -0.09933716, -0.06265577])}\n",
      "ave reward of node  44 =  0.32875954195548307\n",
      "policy of of node  44 =  {(0, 1): array([-2.08072016, -0.36202545,  4.44274561]), (1, 1): array([-1.94147573, -0.22379768,  4.16527341]), (1, 0): array([-1.57099604, -0.14938878,  3.72038482]), (0, 0): array([0.52923768, 0.59463376, 0.87612856])}\n",
      "ave reward of node  45 =  0.04321919590397081\n",
      "policy of of node  45 =  {(1, 0): array([ 3.43976572, -1.37338896, -0.06637675]), (0, 1): array([ 0.06036311, -1.8281917 ,  3.76782859]), (1, 1): array([ 0.25462523, -2.23478309,  3.98015786]), (0, 0): array([ 2.12545613, -0.118913  , -0.00654313])}\n",
      "ave reward of node  46 =  0.14968687096872405\n",
      "policy of of node  46 =  {(0, 0): array([0.65817345, 0.12204128, 1.21978527]), (0, 1): array([-1.34896053, -1.3503594 ,  4.69931993]), (1, 0): array([-0.88323993, -1.0083618 ,  3.89160172]), (1, 1): array([ 0.81277925, -0.17293591,  1.36015666])}\n",
      "ave reward of node  47 =  0.08476247688018737\n",
      "policy of of node  47 =  {(0, 0): array([ 2.52627864, -0.08605688, -0.44022176]), (0, 1): array([-0.16965031,  3.71713553, -1.54748522]), (1, 0): array([ 2.87256075,  0.20127197, -1.07383273]), (1, 1): array([ 2.48270036, -0.0569134 , -0.42578696])}\n",
      "ave reward of node  48 =  0.2371144098251185\n",
      "policy of of node  48 =  {(1, 0): array([-1.68928229,  3.68928229]), (0, 1): array([-2.22719379,  4.22719379]), (0, 0): array([ 2.57989478, -0.57989478]), (1, 1): array([-1.21980944,  3.21980944])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "policyRewardSmoothList = []\n",
    "policyRewardMuList = []\n",
    "finalRewardList = []\n",
    "aveRewardMu = []\n",
    "for k in range(minK,maxK+1):\n",
    "    policyRewardSmooth, policyRewardMu = networkRLModel.train(k = k, M = M, evalInterval = evalInterval,restartIntervalQ = restartIntervalQ\\\n",
    "                                 , restartIntervalPolicy = restartIntervalPolicy, clearPolicy = True)\n",
    "    tmp = 0.0\n",
    "    for i in range(nodeNum):\n",
    "        print('ave reward of node ',str(i), '= ',str(networkRLModel.nodeList[i].mu))\n",
    "        print('policy of of node ', str(i), '= ',networkRLModel.nodeList[i].paramsDict)\n",
    "        tmp+=networkRLModel.nodeList[i].mu\n",
    "    aveRewardMu.append(tmp/nodeNum)\n",
    "    policyRewardSmoothList.append(policyRewardSmooth)\n",
    "    policyRewardMuList.append(policyRewardMu)\n",
    "    finalRewardList.append(policyRewardSmooth[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward calculated by mu =  [0.16083564496538508, 0.1452679076725471]\n",
      "average reward calculated by averaging =  [0.16297786720321933, 0.1454440931302098]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$t$')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('font', size=14)  \n",
    "\n",
    "\n",
    "print('average reward calculated by mu = ', aveRewardMu)\n",
    "\n",
    "print('average reward calculated by averaging = ', finalRewardList)\n",
    "legendStr = []\n",
    "for kInd in range(minK,maxK+1):\n",
    "    plt.plot(np.linspace(1,M,len(policyRewardSmooth)), policyRewardSmoothList[kInd-minK])\n",
    "    if kInd == 0:\n",
    "        legendStr.append('Independent Learner')\n",
    "    else:\n",
    "        legendStr.append('Scalable Actor Critic')\n",
    "\n",
    "plt.plot(np.linspace(1,M,len(policyRewardSmooth)), benchMarkReward*np.ones(len(policyRewardSmooth),dtype = float), linestyle = ':')\n",
    "legendStr.append('Benchmark')\n",
    "plt.xlim(0,M)\n",
    "plt.ylim(0.05,.4)\n",
    "plt.legend(legendStr)\n",
    "plt.ylabel('Average Reward')\n",
    "plt.xlabel('$t$')\n",
    "#plt.savefig('neuripsfinal_grid36_k01.png',bbox_inches='tight')\n",
    "\n",
    "# plt.show()\n",
    "# for kInd in range(maxK+1):\n",
    "#     plt.plot(np.linspace(1,M,len(policyRewardMu)), policyRewardMuList[kInd])\n",
    "#     legendStr.append('$\\kappa = '+str(kInd)+'$')\n",
    "# plt.legend(legendStr)\n",
    "#plt.xlim((M/2,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
