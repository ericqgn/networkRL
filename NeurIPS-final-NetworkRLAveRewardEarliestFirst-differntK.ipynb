{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from scipy import special\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalNetwork:\n",
    "    def __init__(self, nodeNum, k):\n",
    "        self.nodeNum = nodeNum #the total number of nodes in this network\n",
    "        self.adjacencyMatrix = np.eye(self.nodeNum, dtype = int) #initialize the adjacency matrix of the global network\n",
    "        self.k = k #the number of hops used in learning\n",
    "        self.adjacencyMatrixPower = [np.eye(self.nodeNum, dtype = int)] #cache the powers of the adjacency matrix\n",
    "        self.neighborDict = {} #use a hashmap to store the ((node, dist), neighbors) pairs which we have computed\n",
    "        self.addingEdgesFinished = False #have we finished adding edges?\n",
    "    \n",
    "    #add an undirected edge between node i and j\n",
    "    def addEdge(self, i, j):\n",
    "        self.adjacencyMatrix[i, j] = 1\n",
    "        self.adjacencyMatrix[j, i] = 1\n",
    "    \n",
    "    #finish adding edges, so we can construct the k-hop neighborhood after adding edges\n",
    "    def finishAddingEdges(self):\n",
    "        temp = np.eye(self.nodeNum, dtype = int)\n",
    "        #the d-hop adjacency matrix is stored in self.adjacencyMatrixPower[d]\n",
    "        for _ in range(self.k):\n",
    "            temp = np.matmul(temp, self.adjacencyMatrix)\n",
    "            self.adjacencyMatrixPower.append(temp)\n",
    "        self.addingEdgesFinished = True\n",
    "        print(self.adjacencyMatrixPower)\n",
    "    \n",
    "    #query the d-hop neighborhood of node i, return a list of node indices.\n",
    "    def findNeighbors(self, i, d):\n",
    "        if not self.addingEdgesFinished:\n",
    "            print(\"Please finish adding edges before call findNeighbors!\")\n",
    "            return -1\n",
    "        if (i, d) in self.neighborDict: #if we have computed the answer before, return it\n",
    "            return self.neighborDict[(i, d)]\n",
    "        neighbors = []\n",
    "        for j in range(self.nodeNum):\n",
    "            if self.adjacencyMatrixPower[d][i, j] > 0: #this element > 0 implies that dist(i, j) <= d\n",
    "                neighbors.append(j)\n",
    "        self.neighborDict[(i, d)] = neighbors #cache the answer so we can reuse later\n",
    "        return neighbors\n",
    "\n",
    "class AccessNetwork(GlobalNetwork):\n",
    "    def __init__(self, nodeNum, k, accessNum):\n",
    "        super(AccessNetwork, self).__init__(nodeNum, k)\n",
    "        self.accessNum = accessNum\n",
    "        self.accessMatrix = np.zeros((nodeNum, accessNum), dtype = int)\n",
    "    \n",
    "    #add an access point a for node i\n",
    "    def addAccess(self, i, a):\n",
    "        self.accessMatrix[i, a] = 1\n",
    "        \n",
    "    #finish adding access points. we can construct the neighbor graph\n",
    "    def finishAddingAccess(self):\n",
    "        #use accessMatrix to construct the adjacency matrix of (user) nodes\n",
    "        self.adjacencyMatrix = np.matmul(self.accessMatrix, np.transpose(self.accessMatrix))\n",
    "        \n",
    "        #calculate the number of users sharing each access point\n",
    "        self.numNodePerAccess = np.sum(self.accessMatrix,axis = 0)\n",
    "        \n",
    "        super(AccessNetwork, self).finishAddingEdges()\n",
    "    \n",
    "    #find the access points for node i\n",
    "    def findAccess(self, i):\n",
    "        accessPoints = []\n",
    "        for j in range(self.accessNum):\n",
    "            if self.accessMatrix[i, j] > 0:\n",
    "                accessPoints.append(j)\n",
    "        return accessPoints\n",
    "    def setTransmitProb(self,transmitProb):\n",
    "        self.transmitProb = transmitProb\n",
    "        \n",
    "\n",
    "class Node:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        self.state = [] #The list of local state at different time steps\n",
    "        self.action = [] #The list of local actions at different time steps\n",
    "        self.reward = [] #The list of local actions at different time steps\n",
    "        self.currentTimeStep = 0 #Record the current time step.\n",
    "        self.paramsDict = {} #use a hash map to query the parameters given a state (or neighbors' states)\n",
    "        self.QDict = {} #use a hash map to query to the Q value given a (state, action) pair\n",
    "        self.kHop = [] #The list to record the (state, action) pairs of k-hop neighbors\n",
    "    #get the local Q at timeStep\n",
    "    def getQ(self, kHopStateAction):\n",
    "        #if the Q value of kHopStateAction hasn't been queried before, return 0.0 (initial value)\n",
    "        return self.QDict.get(kHopStateAction, 0.0)\n",
    "    \n",
    "    #initialize the local state\n",
    "    def initializeState(self):\n",
    "        pass\n",
    "    #update the local state, it may depends on the states of other nodes at the last time step.\n",
    "    #Remember to increase self.currentTimeStep by 1\n",
    "    def updateState(self):\n",
    "        pass\n",
    "    #update the local action\n",
    "    def updateAction(self):\n",
    "        pass\n",
    "    #update the local reward\n",
    "    def updateReward(self):\n",
    "        pass\n",
    "    #update the local Q value\n",
    "    def updateQ(self):\n",
    "        pass\n",
    "    #update the local parameter\n",
    "    def updateParams(self):\n",
    "        pass\n",
    "    #clear the record. Called when a new inner loop starts. \n",
    "    def restart(self, clearPolicy = True):\n",
    "        self.state.clear()\n",
    "        self.action.clear()\n",
    "        self.reward.clear()\n",
    "        if clearPolicy == True:\n",
    "            self.paramsDict.clear()\n",
    "        \n",
    "        self.kHop = []\n",
    "        self.currentTimeStep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructLinearNetwork(nodeNum, k, nodePerBlock , transmitProb = 'allone'):\n",
    "    #compute the number of access points\n",
    "    accessNum = (nodeNum - 1)//nodePerBlock\n",
    "        \n",
    "    if accessNum <= 0:\n",
    "        print(\"nodeNum is not large enough!\")\n",
    "        return null\n",
    "    accessNetwork = AccessNetwork(nodeNum = nodeNum, k = k, accessNum = accessNum)\n",
    "    for i in range(nodeNum):\n",
    "        j = i//nodePerBlock\n",
    "        if j >= 1:\n",
    "            accessNetwork.addAccess(i, j - 1)\n",
    "        if j < accessNum:\n",
    "            accessNetwork.addAccess(i, j)\n",
    "    accessNetwork.finishAddingAccess()\n",
    "    \n",
    "    \n",
    "    # setting transmitProb \n",
    "    if transmitProb == 'allone':\n",
    "        transmitProb = np.ones(accessNum)\n",
    "    elif transmitProb == 'random':\n",
    "        transmitProb = np.random.rand(accessNum)\n",
    "    \n",
    "    accessNetwork.setTransmitProb(transmitProb)\n",
    "    return accessNetwork\n",
    "\n",
    "def constructGridNetwork(nodeNum, width, height, k, nodePerGrid, transmitProb = 'allone'):\n",
    "    if nodeNum != width * height * nodePerGrid:\n",
    "        print(\"nodeNum does not satisfy the requirement of grid network!\", nodeNum, width, height, nodePerGrid)\n",
    "        return null\n",
    "    accessNum = (width - 1) * (height - 1)\n",
    "    accessNetwork = AccessNetwork(nodeNum = nodeNum, k = k, accessNum = accessNum)\n",
    "    for j in range(accessNum):\n",
    "        upperLeft = j//(width - 1) * width + j%(width - 1)\n",
    "        upperRight = upperLeft + 1\n",
    "        lowerLeft = upperLeft + width\n",
    "        lowerRight = lowerLeft + 1\n",
    "        for a in [upperLeft, upperRight, lowerLeft, lowerRight]:\n",
    "            for b in range(nodePerGrid):\n",
    "                accessNetwork.addAccess(nodePerGrid * a + b, j)\n",
    "    accessNetwork.finishAddingAccess()\n",
    "    \n",
    "    # setting transmitProb \n",
    "    if transmitProb == 'allone':\n",
    "        transmitProb = np.ones(accessNum)\n",
    "    elif transmitProb == 'random':\n",
    "        transmitProb = np.random.rand(accessNum)\n",
    "    \n",
    "    accessNetwork.setTransmitProb(transmitProb)\n",
    "\n",
    "    return accessNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class accessNodeAve(Node):\n",
    "    def __init__(self, index, ddl, arrivalProb, accessNetwork):\n",
    "        super(accessNodeAve, self).__init__(index)\n",
    "        self.ddl = ddl #the initial deadline of each packet\n",
    "        self.arrivalProb = arrivalProb #the arrival probability at each timestep\n",
    "        self.gamma = 1.0 #the discount factor\n",
    "        #we use packetQueue to represent the current local state, which is (e_1, e_2, ..., e_d)\n",
    "        self.packetQueue = np.zeros(self.ddl, dtype = int) #use 1 to represent a packet with this remaining time, otherwise 0\n",
    "        self.accessPoints = accessNetwork.findAccess(i=index) #find and cache the access points this node can access\n",
    "        self.accessNum = len(self.accessPoints) #the number of access points\n",
    "        self.actionNum = self.accessNum  + 1 #the number of possible actions\n",
    "        self.stateNum = 2** self.ddl # number of possible states\n",
    "        \n",
    "        self.mu = 0.0 # estimate for average reward\n",
    "        self.dummyState = tuple(np.zeros(self.ddl, dtype = int))\n",
    "        self.dummyAction = -1\n",
    "        #print('self.dummyStateAction' , self.dummyStateAction)\n",
    "        #construct a list of possible actions\n",
    "        self.actionList = [-1] #(-1, -1) is an empty action that does nothing\n",
    "        for a in self.accessPoints:\n",
    "            self.actionList.append( a)\n",
    "    #remove the first element in packetQueue, and add packetState to the end\n",
    "    def rotateAdd(self, packetState):\n",
    "        #print('self.packetQueue[1:] =',self.packetQueue[1:],'self.ddl = ',self.ddl, 'packetState = ',packetState )\n",
    "        self.packetQueue = np.insert(self.packetQueue[1:], self.ddl - 1, packetState)\n",
    "        #print('new = ',self.packetQueue)\n",
    "        \n",
    "    #initialize the local state (called at the beginning of the training process)\n",
    "    def initializeState(self):\n",
    "        newPacketState = np.random.binomial(1, self.arrivalProb) #Is there a packer arriving at time step 0?\n",
    "        self.rotateAdd(newPacketState) #get the packet queue at time step 0\n",
    "        self.state.append(tuple(self.packetQueue)) #append this state to state record\n",
    "    \n",
    "    #At each time step t, call updateState, updateAction, updateReward, updateQ in this order\n",
    "    def updateState(self):\n",
    "        self.currentTimeStep += 1\n",
    "        lastAction = self.action[-1]\n",
    "        \n",
    "        # find the earliest slot\n",
    "        nonEmptySlots = np.nonzero(self.packetQueue == 1)\n",
    "        \n",
    "        if len(nonEmptySlots) >0: # queue not empty\n",
    "            #if the reward at the last time step is positive, we have successfully send out a packet\n",
    "            if self.reward[-1] > 1 - 1e-3:\n",
    "                self.packetQueue[nonEmptySlots[0]] = 0 # earliest packet is sent\n",
    "        \n",
    "        #sample whether next packet comes\n",
    "        newPacketState = np.random.binomial(1, self.arrivalProb) #Is there a packer arriving at time step 0?\n",
    "        self.rotateAdd(newPacketState) #get the packet queue at time step 0\n",
    "        self.state.append(tuple(self.packetQueue)) #append this state to state record\n",
    "        if len(self.state)>2:\n",
    "            self.state.pop(0)# only keep current\n",
    "            \n",
    "        \n",
    "    def updateAction(self):\n",
    "        # get the current state\n",
    "        currentState = tuple(self.packetQueue)\n",
    "        # fetch the params based on the current state. If haven't updated before, return all zeros\n",
    "        params = self.paramsDict.get(currentState, np.zeros(self.actionNum))\n",
    "        # compute the probability vector\n",
    "        probVec = special.softmax(params)\n",
    "        # randomly select an action based on probVec\n",
    "        currentAction = self.actionList[np.random.choice(a = self.actionNum, p = probVec)]\n",
    "        \n",
    "        self.action.append(currentAction)\n",
    "        if(len(self.action)>2):\n",
    "            self.action.pop(0) # through away the first one. Only keep current and last action\n",
    "    \n",
    "    #oneHopNeighbors is a list of accessNodes\n",
    "    def updateReward(self, oneHopNeighbors, accessNetwork):\n",
    "        #decide if a packet is successfully sending out\n",
    "        currentAction = self.action[-1]\n",
    "        if currentAction == -1: # the do nothing action\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "        currentState = np.array(self.state[-1])\n",
    "        \n",
    "        #check if the node try to send out an empty slot\n",
    "        if np.all(currentState == 0): # if the current queue is empty\n",
    "            # zero reward\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "\n",
    "        \n",
    "        for neighbor in oneHopNeighbors:\n",
    "            if neighbor.index == self.index:\n",
    "                continue\n",
    "            neighborAction = neighbor.action[-1]\n",
    "            \n",
    "            if neighborAction != currentAction: \n",
    "                continue\n",
    "            else:\n",
    "                neighborState = np.array(neighbor.state[-1])\n",
    "                #print('neighborState', neighborState)\n",
    "                if np.any(neighborState == 1): # neighbor queue non empty, conflict!\n",
    "                    #print('conflict!')\n",
    "                    self.reward.append(0.0)\n",
    "                    if(len(self.reward)>2):\n",
    "                        self.reward.pop(0)\n",
    "                    return\n",
    "        \n",
    "        # no conflict, send\n",
    "        transmitSuccess = np.random.binomial(1, accessNetwork.transmitProb[currentAction])\n",
    "        if transmitSuccess == 1:\n",
    "            self.reward.append(1.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "        else:\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "\n",
    "    def isDummyStateAction(self,state,action):\n",
    "        if state == self.dummyState and action == self.dummyAction:\n",
    "            #print(self.dummyState,'==',state,', ',self.dummyAction,'==',action)\n",
    "            return True\n",
    "        else:\n",
    "            #print(self.dummyState,'!=',state,', ',self.dummyAction,'!=',action)\n",
    "            return False\n",
    "                                \n",
    "    \n",
    "    #kHopNeighbors is a list of accessNodes, alpha is learning rate\n",
    "    def updateQ(self, kHopNeighbors, alpha):\n",
    "        lastStateAction = []\n",
    "        currentStateAction = []\n",
    "        #construct a list of the state-action pairs of k-hop neighbors\n",
    "        dummyFlag = []\n",
    "        for neighbor in kHopNeighbors:\n",
    "            neighborLastState = neighbor.state[-2]\n",
    "            neighborCurrentState = neighbor.state[-1]\n",
    "            neighborLastAction = neighbor.action[-2]\n",
    "            neighborCurrentAction = neighbor.action[-1]\n",
    "            lastStateAction.append((neighborLastState, neighborLastAction))\n",
    "            currentStateAction.append((neighborCurrentState, neighborCurrentAction))\n",
    "            dummyFlag.append(neighbor.isDummyStateAction(neighborLastState,neighborLastAction))\n",
    "        lastStateAction = tuple(lastStateAction)\n",
    "        currentStateAction = tuple(currentStateAction)\n",
    "        #fetch the Q value based on neighbors' states and actions\n",
    "        lastQTerm1 = self.QDict.get(lastStateAction, 0.0)\n",
    "        lastQTerm2 = self.QDict.get(currentStateAction, 0.0)\n",
    "        #compute the temporal difference\n",
    "        \n",
    "        self.mu = (1-alpha) * self.mu + alpha* self.reward[-1] # compute new mu\n",
    "        \n",
    "        \n",
    "        temporalDiff = self.reward[-2] - self.mu+  lastQTerm2 - lastQTerm1\n",
    "        #print('lastStateAction',lastStateAction)\n",
    "        #perform the Q value update when last state action is not dummy\n",
    "        if not all(dummyFlag):\n",
    "            self.QDict[lastStateAction] = lastQTerm1 + alpha * temporalDiff\n",
    "            #print(self.QDict[lastStateAction])\n",
    "        # if this time step 1, we should also put lastStateAction into history record\n",
    "        \n",
    "        #if len(self.kHop) == 0:\n",
    "            #self.kHop.append(lastStateAction)\n",
    "        #put currentStateAction into history record\n",
    "        self.kHop = currentStateAction\n",
    "    \n",
    "    #eta is the learning rate\n",
    "    def updateParams(self, kHopNeighbors, eta):\n",
    "        #for t = 0, 1, ..., T, compute the term in g_{i, t}(m) before \\nabla\n",
    "        mutiplier1 = 0.0\n",
    "        t = self.currentTimeStep\n",
    "        #print(' t = ', t,', khop = ', self.kHop)\n",
    "        for neighbor in kHopNeighbors:\n",
    "            neighborKHop = neighbor.kHop\n",
    "            neighborQ = neighbor.getQ(neighborKHop)\n",
    "            mutiplier1 += neighborQ\n",
    "        \n",
    "        mutiplier1 /= nodeNum\n",
    "        #finish constructing mutiplier1\n",
    "        #compute the gradient with respect to the parameters associated with s_i(t)\n",
    "        currentState = self.state[-1]\n",
    "        currentAction = self.action[-1]\n",
    "        \n",
    "        defaultPolicy = np.zeros(self.actionNum)\n",
    "        defaultPolicy[0]=2\n",
    "        \n",
    "        # for test only!!!\n",
    "#         if self.index == 0:\n",
    "#             defaultPolicy[1]=-1\n",
    "        # End of Test!!!\n",
    "        \n",
    "        \n",
    "        params = self.paramsDict.get(currentState, defaultPolicy)\n",
    "        probVec = special.softmax(params)\n",
    "        grad = -probVec\n",
    "        actionIndex = self.actionList.index(currentAction) #get the index of currentAction\n",
    "        grad[actionIndex] += 1.0\n",
    "        self.paramsDict[currentState] = params + eta * mutiplier1 * grad\n",
    "        #print('t=',t ,'  id = ', self.index, '  current state=', len(currentState),' grad = ',grad, 'mutiplier1 =', mutiplier1, 'params=', params)\n",
    "\n",
    "    def setBenchmarkPolicy(self,accessNetwork): # set a naive benchmarkPolicy\n",
    "        proportionAction = []\n",
    "        for actionCounter in range(self.actionNum):\n",
    "            if self.actionList[actionCounter] == -1:\n",
    "                proportionAction.append(np.log(100*.11/4.0))\n",
    "            else:\n",
    "                numNodePerAccess = float(accessNetwork.numNodePerAccess[self.actionList[actionCounter]])\n",
    "                transmitProb = float(accessNetwork.transmitProb[self.actionList[actionCounter]])\n",
    "                print('numNodePerAccess = ',numNodePerAccess,' transmitProb = ',transmitProb)\n",
    "                proportionAction.append( np.log(100*transmitProb/numNodePerAccess))\n",
    "            \n",
    "        \n",
    "        for stateInt in range(self.stateNum): # enumerate state\n",
    "            currentState = self.int2state(stateInt) # turn state integer into binary list\n",
    "            actionParams = np.ones(self.actionNum,dtype = float) * (-10) # default to be all negative\n",
    "            \n",
    "            \n",
    "            if np.all( currentState == 0): # no packet in queue\n",
    "                actionParams[0] = 10.0 # do nothing\n",
    "            else:\n",
    "                actionParams = np.array(proportionAction) # proportional action\n",
    "            # update paramsDict\n",
    "            self.paramsDict[tuple(currentState)] = actionParams\n",
    "                \n",
    "            \n",
    "        \n",
    "    def int2state(self,stateInt):\n",
    "        currentState = np.zeros(self.ddl,dtype = int)\n",
    "        stateIntIterate = stateInt\n",
    "        for i in range(ddl):\n",
    "            currentState[i] = stateIntIterate% self.ddl\n",
    "            stateIntIterate = stateIntIterate//self.ddl\n",
    "        return currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAccessNetworkRL:\n",
    "    def __init__(self,ddl = 2, graphType = 'line', nodeNum = 10, maxK = 3 ,arrivalProb = None, transmitProb = 'random', gridW = 1, gridH = 1):\n",
    "        self.ddl = ddl\n",
    "        self.nodeNum = nodeNum\n",
    "        self.maxK = maxK\n",
    "        if(arrivalProb == None):\n",
    "            self.arrivalProb = np.random.rand(nodeNum)\n",
    "        else:\n",
    "            self.arrivalProb = arrivalProb\n",
    "            \n",
    "        if graphType == 'line':\n",
    "            self.accessNetwork = constructLinearNetwork(nodeNum = nodeNum, nodePerBlock = 1, k = maxK,transmitProb= transmitProb)\n",
    "        else:\n",
    "            self.accessNetwork =  constructGridNetwork(nodeNum = nodeNum, width = gridW, height = gridH, k = maxK, nodePerGrid = 1,transmitProb= transmitProb)\n",
    "            \n",
    "        self.nodeList = []\n",
    "        for i in range(nodeNum):\n",
    "            self.nodeList.append(accessNodeAve(index = i, ddl = ddl, arrivalProb = self.arrivalProb[i], accessNetwork = self.accessNetwork) )\n",
    "       \n",
    "    def train(self, k = 1, M = 10000, evalInterval = 500, restartIntervalQ = 50, restartIntervalPolicy = 50, clearPolicy = True):\n",
    "        for i in range(self.nodeNum):\n",
    "            self.nodeList[i].restart(clearPolicy)\n",
    "            self.nodeList[i].initializeState()\n",
    "            self.nodeList[i].updateAction()\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            neighborList = []\n",
    "            for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                neighborList.append(self.nodeList[j])\n",
    "            self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "            \n",
    "        \n",
    "        policyRewardList = []\n",
    "        policyRewardSmooth = []\n",
    "        policyRewardMuSmooth = []\n",
    "        policyRewardMu = []\n",
    "        for m in trange(M):\n",
    "            tmpReward = 0.0\n",
    "            tmpRewardMu = 0.0\n",
    "            for i in range(self.nodeNum): #update state-action\n",
    "                self.nodeList[i].updateState()\n",
    "                self.nodeList[i].updateAction()\n",
    "            \n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "                \n",
    "                tmpReward += self.nodeList[i].reward[-1] # add latest reward\n",
    "                tmpRewardMu += self.nodeList[i].mu\n",
    "                \n",
    "            policyRewardList.append(tmpReward/self.nodeNum)\n",
    "            policyRewardMu.append(tmpRewardMu/self.nodeNum)\n",
    "            \n",
    "            if m%evalInterval == evalInterval - 1:\n",
    "                policyRewardSmooth.append(np.mean(policyRewardList[-evalInterval+2:-1]) )\n",
    "                policyRewardMuSmooth.append(np.mean(policyRewardMu[-evalInterval+2:-1]) )\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, k):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateQ(neighborList, 1/pow((m%restartIntervalQ)+1,.4)  )\n",
    "\n",
    "            #perform the grad update\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, k):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateParams(neighborList,  1.0* 1/pow((m%restartIntervalPolicy)+1,.6))\n",
    "                \n",
    "            #print(m%restartIntervalPolicy)\n",
    "            if m > M*0.9: # for the last 10% of running, no restarting\n",
    "                restartIntervalQ = max(int(M*0.5),restartIntervalQ)\n",
    "                restartIntervalPolicy = max(int(M*0.5),restartIntervalPolicy)\n",
    "            #perform a policy evaluation\n",
    "        \n",
    "        return policyRewardSmooth, policyRewardMuSmooth\n",
    "    \n",
    "    def evaluateBenchmarkPolicy(self, M=10000):\n",
    "\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            self.nodeList[i].restart()\n",
    "            self.nodeList[i].setBenchmarkPolicy(self.accessNetwork)\n",
    "            self.nodeList[i].initializeState()\n",
    "            self.nodeList[i].updateAction()\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            neighborList = []\n",
    "            for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                neighborList.append(self.nodeList[j])\n",
    "            self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "\n",
    "        policyRewardList = []\n",
    "\n",
    "        for m in trange(M):\n",
    "            tmpReward = 0.0\n",
    "            for i in range(self.nodeNum): #update state-action\n",
    "                self.nodeList[i].updateState()\n",
    "                self.nodeList[i].updateAction()\n",
    "\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "                tmpReward += self.nodeList[i].reward[-1] # add latest reward\n",
    "            policyRewardList.append(tmpReward/self.nodeNum)\n",
    "\n",
    "        return np.mean(policyRewardList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = 2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# linear network\n",
    "# nodeNum = 5 #number of nodes in the network\n",
    "# maxK = 2 #the size of neighborhood we use in localized learning\n",
    "# arrivalProb = [.3,.9, .9] *2\n",
    "# transmitProb = [ .9,.9]   + [.02, .9, .9 ]*1\n",
    "# networkRLModel = MultiAccessNetworkRL(ddl = 2, graphType = 'line', \\\n",
    "#                                       nodeNum = nodeNum, maxK = maxK ,arrivalProb = arrivalProb,transmitProb = transmitProb)\n",
    "# evalInterval = 500 #evaluate the policy every evalInterval rounds (outer loop)\n",
    "# M = 10000\n",
    "# restartIntervalQ = 500;\n",
    "# restartIntervalPolicy = 500\n",
    "\n",
    "\n",
    "gridW = 6\n",
    "gridH = 6\n",
    "nodeNum = gridW*gridH #number of nodes in the network\n",
    "minK = 0\n",
    "maxK = 1 #the size of neighborhood we use in localized learning\n",
    "arrivalProb = None\n",
    "transmitProb = 'random'\n",
    "networkRLModel = MultiAccessNetworkRL(ddl = 2, graphType = 'grid', \\\n",
    "                                      nodeNum = nodeNum, maxK = maxK ,arrivalProb = arrivalProb,transmitProb = transmitProb,gridW = gridW, gridH = gridH)\n",
    "evalInterval = 500 #evaluate the policy every evalInterval rounds (outer loop)\n",
    "M = 300000\n",
    "restartIntervalQ = 10\n",
    "restartIntervalPolicy = 5\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchMarkReward = networkRLModel.evaluateBenchmarkPolicy()\n",
    "\n",
    "print('benchmark reward is: ', benchMarkReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "policyRewardSmoothList = []\n",
    "policyRewardMuList = []\n",
    "finalRewardList = []\n",
    "aveRewardMu = []\n",
    "for k in range(minK,maxK+1):\n",
    "    policyRewardSmooth, policyRewardMu = networkRLModel.train(k = k, M = M, evalInterval = evalInterval,restartIntervalQ = restartIntervalQ\\\n",
    "                                 , restartIntervalPolicy = restartIntervalPolicy, clearPolicy = True)\n",
    "    tmp = 0.0\n",
    "    for i in range(nodeNum):\n",
    "        print('ave reward of node ',str(i), '= ',str(networkRLModel.nodeList[i].mu))\n",
    "        print('policy of of node ', str(i), '= ',networkRLModel.nodeList[i].paramsDict)\n",
    "        tmp+=networkRLModel.nodeList[i].mu\n",
    "    aveRewardMu.append(tmp/nodeNum)\n",
    "    policyRewardSmoothList.append(policyRewardSmooth)\n",
    "    policyRewardMuList.append(policyRewardMu)\n",
    "    finalRewardList.append(policyRewardSmooth[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)  \n",
    "\n",
    "\n",
    "print('average reward calculated by mu = ', aveRewardMu)\n",
    "\n",
    "print('average reward calculated by averaging = ', finalRewardList)\n",
    "legendStr = []\n",
    "for kInd in range(minK,maxK+1):\n",
    "    plt.plot(np.linspace(1,M,len(policyRewardSmooth)), policyRewardSmoothList[kInd-minK])\n",
    "    if kInd == 0:\n",
    "        legendStr.append('Independent Learner')\n",
    "    else:\n",
    "        legendStr.append('Scalable Actor Critic')\n",
    "\n",
    "plt.plot(np.linspace(1,M,len(policyRewardSmooth)), benchMarkReward*np.ones(len(policyRewardSmooth),dtype = float), linestyle = ':')\n",
    "legendStr.append('Benchmark')\n",
    "plt.xlim(0,M)\n",
    "plt.ylim(0.05,.4)\n",
    "plt.legend(legendStr)\n",
    "plt.ylabel('Average Reward')\n",
    "plt.xlabel('$t$')\n",
    "#plt.savefig('neuripsfinal_grid36_k01.png',bbox_inches='tight')\n",
    "\n",
    "# plt.show()\n",
    "# for kInd in range(maxK+1):\n",
    "#     plt.plot(np.linspace(1,M,len(policyRewardMu)), policyRewardMuList[kInd])\n",
    "#     legendStr.append('$\\kappa = '+str(kInd)+'$')\n",
    "# plt.legend(legendStr)\n",
    "#plt.xlim((M/2,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
