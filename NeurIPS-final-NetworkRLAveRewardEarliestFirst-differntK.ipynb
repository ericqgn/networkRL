{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from scipy import special\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalNetwork:\n",
    "    def __init__(self, nodeNum, k):\n",
    "        self.nodeNum = nodeNum #the total number of nodes in this network\n",
    "        self.adjacencyMatrix = np.eye(self.nodeNum, dtype = int) #initialize the adjacency matrix of the global network\n",
    "        self.k = k #the number of hops used in learning\n",
    "        self.adjacencyMatrixPower = [np.eye(self.nodeNum, dtype = int)] #cache the powers of the adjacency matrix\n",
    "        self.neighborDict = {} #use a hashmap to store the ((node, dist), neighbors) pairs which we have computed\n",
    "        self.addingEdgesFinished = False #have we finished adding edges?\n",
    "    \n",
    "    #add an undirected edge between node i and j\n",
    "    def addEdge(self, i, j):\n",
    "        self.adjacencyMatrix[i, j] = 1\n",
    "        self.adjacencyMatrix[j, i] = 1\n",
    "    \n",
    "    #finish adding edges, so we can construct the k-hop neighborhood after adding edges\n",
    "    def finishAddingEdges(self):\n",
    "        temp = np.eye(self.nodeNum, dtype = int)\n",
    "        #the d-hop adjacency matrix is stored in self.adjacencyMatrixPower[d]\n",
    "        for _ in range(self.k):\n",
    "            temp = np.matmul(temp, self.adjacencyMatrix)\n",
    "            self.adjacencyMatrixPower.append(temp)\n",
    "        self.addingEdgesFinished = True\n",
    "        print(self.adjacencyMatrixPower)\n",
    "    \n",
    "    #query the d-hop neighborhood of node i, return a list of node indices.\n",
    "    def findNeighbors(self, i, d):\n",
    "        if not self.addingEdgesFinished:\n",
    "            print(\"Please finish adding edges before call findNeighbors!\")\n",
    "            return -1\n",
    "        if (i, d) in self.neighborDict: #if we have computed the answer before, return it\n",
    "            return self.neighborDict[(i, d)]\n",
    "        neighbors = []\n",
    "        for j in range(self.nodeNum):\n",
    "            if self.adjacencyMatrixPower[d][i, j] > 0: #this element > 0 implies that dist(i, j) <= d\n",
    "                neighbors.append(j)\n",
    "        self.neighborDict[(i, d)] = neighbors #cache the answer so we can reuse later\n",
    "        return neighbors\n",
    "\n",
    "class AccessNetwork(GlobalNetwork):\n",
    "    def __init__(self, nodeNum, k, accessNum):\n",
    "        super(AccessNetwork, self).__init__(nodeNum, k)\n",
    "        self.accessNum = accessNum\n",
    "        self.accessMatrix = np.zeros((nodeNum, accessNum), dtype = int)\n",
    "    \n",
    "    #add an access point a for node i\n",
    "    def addAccess(self, i, a):\n",
    "        self.accessMatrix[i, a] = 1\n",
    "        \n",
    "    #finish adding access points. we can construct the neighbor graph\n",
    "    def finishAddingAccess(self):\n",
    "        #use accessMatrix to construct the adjacency matrix of (user) nodes\n",
    "        self.adjacencyMatrix = np.matmul(self.accessMatrix, np.transpose(self.accessMatrix))\n",
    "        \n",
    "        #calculate the number of users sharing each access point\n",
    "        self.numNodePerAccess = np.sum(self.accessMatrix,axis = 0)\n",
    "        \n",
    "        super(AccessNetwork, self).finishAddingEdges()\n",
    "    \n",
    "    #find the access points for node i\n",
    "    def findAccess(self, i):\n",
    "        accessPoints = []\n",
    "        for j in range(self.accessNum):\n",
    "            if self.accessMatrix[i, j] > 0:\n",
    "                accessPoints.append(j)\n",
    "        return accessPoints\n",
    "    def setTransmitProb(self,transmitProb):\n",
    "        self.transmitProb = transmitProb\n",
    "        \n",
    "\n",
    "class Node:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        self.state = [] #The list of local state at different time steps\n",
    "        self.action = [] #The list of local actions at different time steps\n",
    "        self.reward = [] #The list of local actions at different time steps\n",
    "        self.currentTimeStep = 0 #Record the current time step.\n",
    "        self.paramsDict = {} #use a hash map to query the parameters given a state (or neighbors' states)\n",
    "        self.QDict = {} #use a hash map to query to the Q value given a (state, action) pair\n",
    "        self.kHop = [] #The list to record the (state, action) pairs of k-hop neighbors\n",
    "    #get the local Q at timeStep\n",
    "    def getQ(self, kHopStateAction):\n",
    "        #if the Q value of kHopStateAction hasn't been queried before, return 0.0 (initial value)\n",
    "        return self.QDict.get(kHopStateAction, 0.0)\n",
    "    \n",
    "    #initialize the local state\n",
    "    def initializeState(self):\n",
    "        pass\n",
    "    #update the local state, it may depends on the states of other nodes at the last time step.\n",
    "    #Remember to increase self.currentTimeStep by 1\n",
    "    def updateState(self):\n",
    "        pass\n",
    "    #update the local action\n",
    "    def updateAction(self):\n",
    "        pass\n",
    "    #update the local reward\n",
    "    def updateReward(self):\n",
    "        pass\n",
    "    #update the local Q value\n",
    "    def updateQ(self):\n",
    "        pass\n",
    "    #update the local parameter\n",
    "    def updateParams(self):\n",
    "        pass\n",
    "    #clear the record. Called when a new inner loop starts. \n",
    "    def restart(self, clearPolicy = True):\n",
    "        self.state.clear()\n",
    "        self.action.clear()\n",
    "        self.reward.clear()\n",
    "        if clearPolicy == True:\n",
    "            self.paramsDict.clear()\n",
    "        \n",
    "        self.kHop = []\n",
    "        self.currentTimeStep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructLinearNetwork(nodeNum, k, nodePerBlock , transmitProb = 'allone'):\n",
    "    #compute the number of access points\n",
    "    accessNum = (nodeNum - 1)//nodePerBlock\n",
    "        \n",
    "    if accessNum <= 0:\n",
    "        print(\"nodeNum is not large enough!\")\n",
    "        return null\n",
    "    accessNetwork = AccessNetwork(nodeNum = nodeNum, k = k, accessNum = accessNum)\n",
    "    for i in range(nodeNum):\n",
    "        j = i//nodePerBlock\n",
    "        if j >= 1:\n",
    "            accessNetwork.addAccess(i, j - 1)\n",
    "        if j < accessNum:\n",
    "            accessNetwork.addAccess(i, j)\n",
    "    accessNetwork.finishAddingAccess()\n",
    "    \n",
    "    \n",
    "    # setting transmitProb \n",
    "    if transmitProb == 'allone':\n",
    "        transmitProb = np.ones(accessNum)\n",
    "    elif transmitProb == 'random':\n",
    "        transmitProb = np.random.rand(accessNum)\n",
    "    \n",
    "    accessNetwork.setTransmitProb(transmitProb)\n",
    "    return accessNetwork\n",
    "\n",
    "def constructGridNetwork(nodeNum, width, height, k, nodePerGrid, transmitProb = 'allone'):\n",
    "    if nodeNum != width * height * nodePerGrid:\n",
    "        print(\"nodeNum does not satisfy the requirement of grid network!\", nodeNum, width, height, nodePerGrid)\n",
    "        return null\n",
    "    accessNum = (width - 1) * (height - 1)\n",
    "    accessNetwork = AccessNetwork(nodeNum = nodeNum, k = k, accessNum = accessNum)\n",
    "    for j in range(accessNum):\n",
    "        upperLeft = j//(width - 1) * width + j%(width - 1)\n",
    "        upperRight = upperLeft + 1\n",
    "        lowerLeft = upperLeft + width\n",
    "        lowerRight = lowerLeft + 1\n",
    "        for a in [upperLeft, upperRight, lowerLeft, lowerRight]:\n",
    "            for b in range(nodePerGrid):\n",
    "                accessNetwork.addAccess(nodePerGrid * a + b, j)\n",
    "    accessNetwork.finishAddingAccess()\n",
    "    \n",
    "    # setting transmitProb \n",
    "    if transmitProb == 'allone':\n",
    "        transmitProb = np.ones(accessNum)\n",
    "    elif transmitProb == 'random':\n",
    "        transmitProb = np.random.rand(accessNum)\n",
    "    \n",
    "    accessNetwork.setTransmitProb(transmitProb)\n",
    "\n",
    "    return accessNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class accessNodeAve(Node):\n",
    "    def __init__(self, index, ddl, arrivalProb, accessNetwork):\n",
    "        super(accessNodeAve, self).__init__(index)\n",
    "        self.ddl = ddl #the initial deadline of each packet\n",
    "        self.arrivalProb = arrivalProb #the arrival probability at each timestep\n",
    "        self.gamma = 1.0 #the discount factor\n",
    "        #we use packetQueue to represent the current local state, which is (e_1, e_2, ..., e_d)\n",
    "        self.packetQueue = np.zeros(self.ddl, dtype = int) #use 1 to represent a packet with this remaining time, otherwise 0\n",
    "        self.accessPoints = accessNetwork.findAccess(i=index) #find and cache the access points this node can access\n",
    "        self.accessNum = len(self.accessPoints) #the number of access points\n",
    "        self.actionNum = self.accessNum  + 1 #the number of possible actions\n",
    "        self.stateNum = 2** self.ddl # number of possible states\n",
    "        \n",
    "        self.mu = 0.0 # estimate for average reward\n",
    "        self.dummyState = tuple(np.zeros(self.ddl, dtype = int))\n",
    "        self.dummyAction = -1\n",
    "        #print('self.dummyStateAction' , self.dummyStateAction)\n",
    "        #construct a list of possible actions\n",
    "        self.actionList = [-1] #(-1, -1) is an empty action that does nothing\n",
    "        for a in self.accessPoints:\n",
    "            self.actionList.append( a)\n",
    "    #remove the first element in packetQueue, and add packetState to the end\n",
    "    def rotateAdd(self, packetState):\n",
    "        #print('self.packetQueue[1:] =',self.packetQueue[1:],'self.ddl = ',self.ddl, 'packetState = ',packetState )\n",
    "        self.packetQueue = np.insert(self.packetQueue[1:], self.ddl - 1, packetState)\n",
    "        #print('new = ',self.packetQueue)\n",
    "        \n",
    "    #initialize the local state (called at the beginning of the training process)\n",
    "    def initializeState(self):\n",
    "        newPacketState = np.random.binomial(1, self.arrivalProb) #Is there a packer arriving at time step 0?\n",
    "        self.rotateAdd(newPacketState) #get the packet queue at time step 0\n",
    "        self.state.append(tuple(self.packetQueue)) #append this state to state record\n",
    "    \n",
    "    #At each time step t, call updateState, updateAction, updateReward, updateQ in this order\n",
    "    def updateState(self):\n",
    "        self.currentTimeStep += 1\n",
    "        lastAction = self.action[-1]\n",
    "        \n",
    "        # find the earliest slot\n",
    "        nonEmptySlots = np.nonzero(self.packetQueue == 1)\n",
    "        \n",
    "        if len(nonEmptySlots) >0: # queue not empty\n",
    "            #if the reward at the last time step is positive, we have successfully send out a packet\n",
    "            if self.reward[-1] > 1 - 1e-3:\n",
    "                self.packetQueue[nonEmptySlots[0]] = 0 # earliest packet is sent\n",
    "        \n",
    "        #sample whether next packet comes\n",
    "        newPacketState = np.random.binomial(1, self.arrivalProb) #Is there a packer arriving at time step 0?\n",
    "        self.rotateAdd(newPacketState) #get the packet queue at time step 0\n",
    "        self.state.append(tuple(self.packetQueue)) #append this state to state record\n",
    "        if len(self.state)>2:\n",
    "            self.state.pop(0)# only keep current\n",
    "            \n",
    "        \n",
    "    def updateAction(self):\n",
    "        # get the current state\n",
    "        currentState = tuple(self.packetQueue)\n",
    "        # fetch the params based on the current state. If haven't updated before, return all zeros\n",
    "        params = self.paramsDict.get(currentState, np.zeros(self.actionNum))\n",
    "        # compute the probability vector\n",
    "        probVec = special.softmax(params)\n",
    "        # randomly select an action based on probVec\n",
    "        currentAction = self.actionList[np.random.choice(a = self.actionNum, p = probVec)]\n",
    "        \n",
    "        self.action.append(currentAction)\n",
    "        if(len(self.action)>2):\n",
    "            self.action.pop(0) # through away the first one. Only keep current and last action\n",
    "    \n",
    "    #oneHopNeighbors is a list of accessNodes\n",
    "    def updateReward(self, oneHopNeighbors, accessNetwork):\n",
    "        #decide if a packet is successfully sending out\n",
    "        currentAction = self.action[-1]\n",
    "        if currentAction == -1: # the do nothing action\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "        currentState = np.array(self.state[-1])\n",
    "        \n",
    "        #check if the node try to send out an empty slot\n",
    "        if np.all(currentState == 0): # if the current queue is empty\n",
    "            # zero reward\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "\n",
    "        \n",
    "        for neighbor in oneHopNeighbors:\n",
    "            if neighbor.index == self.index:\n",
    "                continue\n",
    "            neighborAction = neighbor.action[-1]\n",
    "            \n",
    "            if neighborAction != currentAction: \n",
    "                continue\n",
    "            else:\n",
    "                neighborState = np.array(neighbor.state[-1])\n",
    "                #print('neighborState', neighborState)\n",
    "                if np.any(neighborState == 1): # neighbor queue non empty, conflict!\n",
    "                    #print('conflict!')\n",
    "                    self.reward.append(0.0)\n",
    "                    if(len(self.reward)>2):\n",
    "                        self.reward.pop(0)\n",
    "                    return\n",
    "        \n",
    "        # no conflict, send\n",
    "        transmitSuccess = np.random.binomial(1, accessNetwork.transmitProb[currentAction])\n",
    "        if transmitSuccess == 1:\n",
    "            self.reward.append(1.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "        else:\n",
    "            self.reward.append(0.0)\n",
    "            if(len(self.reward)>2):\n",
    "                self.reward.pop(0)\n",
    "            return\n",
    "\n",
    "    def isDummyStateAction(self,state,action):\n",
    "        if state == self.dummyState and action == self.dummyAction:\n",
    "            #print(self.dummyState,'==',state,', ',self.dummyAction,'==',action)\n",
    "            return True\n",
    "        else:\n",
    "            #print(self.dummyState,'!=',state,', ',self.dummyAction,'!=',action)\n",
    "            return False\n",
    "                                \n",
    "    \n",
    "    #kHopNeighbors is a list of accessNodes, alpha is learning rate\n",
    "    def updateQ(self, kHopNeighbors, alpha):\n",
    "        lastStateAction = []\n",
    "        currentStateAction = []\n",
    "        #construct a list of the state-action pairs of k-hop neighbors\n",
    "        dummyFlag = []\n",
    "        for neighbor in kHopNeighbors:\n",
    "            neighborLastState = neighbor.state[-2]\n",
    "            neighborCurrentState = neighbor.state[-1]\n",
    "            neighborLastAction = neighbor.action[-2]\n",
    "            neighborCurrentAction = neighbor.action[-1]\n",
    "            lastStateAction.append((neighborLastState, neighborLastAction))\n",
    "            currentStateAction.append((neighborCurrentState, neighborCurrentAction))\n",
    "            dummyFlag.append(neighbor.isDummyStateAction(neighborLastState,neighborLastAction))\n",
    "        lastStateAction = tuple(lastStateAction)\n",
    "        currentStateAction = tuple(currentStateAction)\n",
    "        #fetch the Q value based on neighbors' states and actions\n",
    "        lastQTerm1 = self.QDict.get(lastStateAction, 0.0)\n",
    "        lastQTerm2 = self.QDict.get(currentStateAction, 0.0)\n",
    "        #compute the temporal difference\n",
    "        \n",
    "        self.mu = (1-alpha) * self.mu + alpha* self.reward[-1] # compute new mu\n",
    "        \n",
    "        \n",
    "        temporalDiff = self.reward[-2] - self.mu+  lastQTerm2 - lastQTerm1\n",
    "        #print('lastStateAction',lastStateAction)\n",
    "        #perform the Q value update when last state action is not dummy\n",
    "        if not all(dummyFlag):\n",
    "            self.QDict[lastStateAction] = lastQTerm1 + alpha * temporalDiff\n",
    "            #print(self.QDict[lastStateAction])\n",
    "        # if this time step 1, we should also put lastStateAction into history record\n",
    "        \n",
    "        #if len(self.kHop) == 0:\n",
    "            #self.kHop.append(lastStateAction)\n",
    "        #put currentStateAction into history record\n",
    "        self.kHop = currentStateAction\n",
    "    \n",
    "    #eta is the learning rate\n",
    "    def updateParams(self, kHopNeighbors, eta):\n",
    "        #for t = 0, 1, ..., T, compute the term in g_{i, t}(m) before \\nabla\n",
    "        mutiplier1 = 0.0\n",
    "        t = self.currentTimeStep\n",
    "        #print(' t = ', t,', khop = ', self.kHop)\n",
    "        for neighbor in kHopNeighbors:\n",
    "            neighborKHop = neighbor.kHop\n",
    "            neighborQ = neighbor.getQ(neighborKHop)\n",
    "            mutiplier1 += neighborQ\n",
    "        \n",
    "        mutiplier1 /= nodeNum\n",
    "        #finish constructing mutiplier1\n",
    "        #compute the gradient with respect to the parameters associated with s_i(t)\n",
    "        currentState = self.state[-1]\n",
    "        currentAction = self.action[-1]\n",
    "        \n",
    "        defaultPolicy = np.zeros(self.actionNum)\n",
    "        defaultPolicy[0]=2\n",
    "        \n",
    "        # for test only!!!\n",
    "#         if self.index == 0:\n",
    "#             defaultPolicy[1]=-1\n",
    "        # End of Test!!!\n",
    "        \n",
    "        \n",
    "        params = self.paramsDict.get(currentState, defaultPolicy)\n",
    "        probVec = special.softmax(params)\n",
    "        grad = -probVec\n",
    "        actionIndex = self.actionList.index(currentAction) #get the index of currentAction\n",
    "        grad[actionIndex] += 1.0\n",
    "        self.paramsDict[currentState] = params + eta * mutiplier1 * grad\n",
    "        #print('t=',t ,'  id = ', self.index, '  current state=', len(currentState),' grad = ',grad, 'mutiplier1 =', mutiplier1, 'params=', params)\n",
    "\n",
    "    def setBenchmarkPolicy(self,accessNetwork): # set a naive benchmarkPolicy\n",
    "        proportionAction = []\n",
    "        for actionCounter in range(self.actionNum):\n",
    "            if self.actionList[actionCounter] == -1:\n",
    "                proportionAction.append(np.log(100*.11/4.0))\n",
    "            else:\n",
    "                numNodePerAccess = float(accessNetwork.numNodePerAccess[self.actionList[actionCounter]])\n",
    "                transmitProb = float(accessNetwork.transmitProb[self.actionList[actionCounter]])\n",
    "                print('numNodePerAccess = ',numNodePerAccess,' transmitProb = ',transmitProb)\n",
    "                proportionAction.append( np.log(100*transmitProb/numNodePerAccess))\n",
    "            \n",
    "        \n",
    "        for stateInt in range(self.stateNum): # enumerate state\n",
    "            currentState = self.int2state(stateInt) # turn state integer into binary list\n",
    "            actionParams = np.ones(self.actionNum,dtype = float) * (-10) # default to be all negative\n",
    "            \n",
    "            \n",
    "            if np.all( currentState == 0): # no packet in queue\n",
    "                actionParams[0] = 10.0 # do nothing\n",
    "            else:\n",
    "                actionParams = np.array(proportionAction) # proportional action\n",
    "            # update paramsDict\n",
    "            self.paramsDict[tuple(currentState)] = actionParams\n",
    "                \n",
    "            \n",
    "        \n",
    "    def int2state(self,stateInt):\n",
    "        currentState = np.zeros(self.ddl,dtype = int)\n",
    "        stateIntIterate = stateInt\n",
    "        for i in range(ddl):\n",
    "            currentState[i] = stateIntIterate% self.ddl\n",
    "            stateIntIterate = stateIntIterate//self.ddl\n",
    "        return currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAccessNetworkRL:\n",
    "    def __init__(self,ddl = 2, graphType = 'line', nodeNum = 10, maxK = 3 ,arrivalProb = None, transmitProb = 'random', gridW = 1, gridH = 1):\n",
    "        self.ddl = ddl\n",
    "        self.nodeNum = nodeNum\n",
    "        self.maxK = maxK\n",
    "        if(arrivalProb == None):\n",
    "            self.arrivalProb = np.random.rand(nodeNum)\n",
    "        else:\n",
    "            self.arrivalProb = arrivalProb\n",
    "            \n",
    "        if graphType == 'line':\n",
    "            self.accessNetwork = constructLinearNetwork(nodeNum = nodeNum, nodePerBlock = 1, k = maxK,transmitProb= transmitProb)\n",
    "        else:\n",
    "            self.accessNetwork =  constructGridNetwork(nodeNum = nodeNum, width = gridW, height = gridH, k = maxK, nodePerGrid = 1,transmitProb= transmitProb)\n",
    "            \n",
    "        self.nodeList = []\n",
    "        for i in range(nodeNum):\n",
    "            self.nodeList.append(accessNodeAve(index = i, ddl = ddl, arrivalProb = self.arrivalProb[i], accessNetwork = self.accessNetwork) )\n",
    "       \n",
    "    def train(self, k = 1, M = 10000, evalInterval = 500, restartIntervalQ = 50, restartIntervalPolicy = 50, clearPolicy = True):\n",
    "        for i in range(self.nodeNum):\n",
    "            self.nodeList[i].restart(clearPolicy)\n",
    "            self.nodeList[i].initializeState()\n",
    "            self.nodeList[i].updateAction()\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            neighborList = []\n",
    "            for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                neighborList.append(self.nodeList[j])\n",
    "            self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "            \n",
    "        \n",
    "        policyRewardList = []\n",
    "        policyRewardSmooth = []\n",
    "        policyRewardMuSmooth = []\n",
    "        policyRewardMu = []\n",
    "        for m in trange(M):\n",
    "            tmpReward = 0.0\n",
    "            tmpRewardMu = 0.0\n",
    "            for i in range(self.nodeNum): #update state-action\n",
    "                self.nodeList[i].updateState()\n",
    "                self.nodeList[i].updateAction()\n",
    "            \n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "                \n",
    "                tmpReward += self.nodeList[i].reward[-1] # add latest reward\n",
    "                tmpRewardMu += self.nodeList[i].mu\n",
    "                \n",
    "            policyRewardList.append(tmpReward/self.nodeNum)\n",
    "            policyRewardMu.append(tmpRewardMu/self.nodeNum)\n",
    "            \n",
    "            if m%evalInterval == evalInterval - 1:\n",
    "                policyRewardSmooth.append(np.mean(policyRewardList[-evalInterval+2:-1]) )\n",
    "                policyRewardMuSmooth.append(np.mean(policyRewardMu[-evalInterval+2:-1]) )\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, k):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateQ(neighborList, 1/pow((m%restartIntervalQ)+1,.4)  )\n",
    "\n",
    "            #perform the grad update\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, k):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateParams(neighborList,  1.0* 1/pow((m%restartIntervalPolicy)+1,.6))\n",
    "                \n",
    "            #print(m%restartIntervalPolicy)\n",
    "            if m > M*0.9: # for the last 10% of running, no restarting\n",
    "                restartIntervalQ = max(int(M*0.5),restartIntervalQ)\n",
    "                restartIntervalPolicy = max(int(M*0.5),restartIntervalPolicy)\n",
    "            #perform a policy evaluation\n",
    "        \n",
    "        return policyRewardSmooth, policyRewardMuSmooth\n",
    "    \n",
    "    def evaluateBenchmarkPolicy(self, M=10000):\n",
    "\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            self.nodeList[i].restart()\n",
    "            self.nodeList[i].setBenchmarkPolicy(self.accessNetwork)\n",
    "            self.nodeList[i].initializeState()\n",
    "            self.nodeList[i].updateAction()\n",
    "        \n",
    "        for i in range(self.nodeNum):\n",
    "            neighborList = []\n",
    "            for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                neighborList.append(self.nodeList[j])\n",
    "            self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "\n",
    "        policyRewardList = []\n",
    "\n",
    "        for m in trange(M):\n",
    "            tmpReward = 0.0\n",
    "            for i in range(self.nodeNum): #update state-action\n",
    "                self.nodeList[i].updateState()\n",
    "                self.nodeList[i].updateAction()\n",
    "\n",
    "            for i in range(self.nodeNum):\n",
    "                neighborList = []\n",
    "                for j in self.accessNetwork.findNeighbors(i, 1):\n",
    "                    neighborList.append(self.nodeList[j])\n",
    "                self.nodeList[i].updateReward(neighborList,self.accessNetwork)\n",
    "                tmpReward += self.nodeList[i].reward[-1] # add latest reward\n",
    "            policyRewardList.append(tmpReward/self.nodeNum)\n",
    "\n",
    "        return np.mean(policyRewardList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 1, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 1, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 1]]), array([[1, 1, 0, ..., 0, 0, 0],\n",
      "       [1, 2, 1, ..., 0, 0, 0],\n",
      "       [0, 1, 2, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 2, 1, 0],\n",
      "       [0, 0, 0, ..., 1, 2, 1],\n",
      "       [0, 0, 0, ..., 0, 1, 1]])]\n"
     ]
    }
   ],
   "source": [
    "ddl = 2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# linear network\n",
    "# nodeNum = 5 #number of nodes in the network\n",
    "# maxK = 2 #the size of neighborhood we use in localized learning\n",
    "# arrivalProb = [.3,.9, .9] *2\n",
    "# transmitProb = [ .9,.9]   + [.02, .9, .9 ]*1\n",
    "# networkRLModel = MultiAccessNetworkRL(ddl = 2, graphType = 'line', \\\n",
    "#                                       nodeNum = nodeNum, maxK = maxK ,arrivalProb = arrivalProb,transmitProb = transmitProb)\n",
    "# evalInterval = 500 #evaluate the policy every evalInterval rounds (outer loop)\n",
    "# M = 10000\n",
    "# restartIntervalQ = 500;\n",
    "# restartIntervalPolicy = 500\n",
    "\n",
    "\n",
    "gridW = 6\n",
    "gridH = 6\n",
    "nodeNum = gridW*gridH #number of nodes in the network\n",
    "minK = 0\n",
    "maxK = 1 #the size of neighborhood we use in localized learning\n",
    "arrivalProb = None\n",
    "transmitProb = 'random'\n",
    "networkRLModel = MultiAccessNetworkRL(ddl = 2, graphType = 'grid', \\\n",
    "                                      nodeNum = nodeNum, maxK = maxK ,arrivalProb = arrivalProb,transmitProb = transmitProb,gridW = gridW, gridH = gridH)\n",
    "evalInterval = 500 #evaluate the policy every evalInterval rounds (outer loop)\n",
    "M = 300000\n",
    "restartIntervalQ = 10\n",
    "restartIntervalPolicy = 10\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/10000 [00:00<01:38, 101.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodePerAccess =  4.0  transmitProb =  0.6120957227224214\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6120957227224214\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6169339968747569\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6169339968747569\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9437480785146242\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9437480785146242\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6818202991034834\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6818202991034834\n",
      "numNodePerAccess =  4.0  transmitProb =  0.359507900573786\n",
      "numNodePerAccess =  4.0  transmitProb =  0.359507900573786\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6120957227224214\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43703195379934145\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6120957227224214\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6169339968747569\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43703195379934145\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6976311959272649\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6169339968747569\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9437480785146242\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6976311959272649\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06022547162926983\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9437480785146242\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6818202991034834\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06022547162926983\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6667667154456677\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6818202991034834\n",
      "numNodePerAccess =  4.0  transmitProb =  0.359507900573786\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6667667154456677\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6706378696181594\n",
      "numNodePerAccess =  4.0  transmitProb =  0.359507900573786\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6706378696181594\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43703195379934145\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2103825610738409\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43703195379934145\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6976311959272649\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2103825610738409\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1289262976548533\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6976311959272649\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06022547162926983\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1289262976548533\n",
      "numNodePerAccess =  4.0  transmitProb =  0.31542835092418386\n",
      "numNodePerAccess =  4.0  transmitProb =  0.06022547162926983\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6667667154456677\n",
      "numNodePerAccess =  4.0  transmitProb =  0.31542835092418386\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6667667154456677\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6706378696181594\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6706378696181594\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2103825610738409\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2103825610738409\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1289262976548533\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.1289262976548533\n",
      "numNodePerAccess =  4.0  transmitProb =  0.31542835092418386\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.31542835092418386\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.3637107709426226\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.5701967704178796\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.43860151346232035\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.9883738380592262\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.10204481074802807\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2088767560948347\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n",
      "numNodePerAccess =  4.0  transmitProb =  0.16130951788499626\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.6531083254653984\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.2532916025397821\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.4663107728563063\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.24442559200160274\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n",
      "numNodePerAccess =  4.0  transmitProb =  0.15896958364551972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:50<00:00, 197.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark reward is:  0.12282222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchMarkReward = networkRLModel.evaluateBenchmarkPolicy()\n",
    "\n",
    "print('benchmark reward is: ', benchMarkReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [35:02<00:00, 142.72it/s]\n",
      "  0%|          | 12/300000 [00:00<43:54, 113.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave reward of node  0 =  0.16269056399523685\n",
      "policy of of node  0 =  {(0, 0): array([ 3.03447088, -1.03447088]), (0, 1): array([-2.38214703,  4.38214703]), (1, 1): array([-1.7668273,  3.7668273]), (1, 0): array([-1.7725779,  3.7725779])}\n",
      "ave reward of node  1 =  0.0725758302847394\n",
      "policy of of node  1 =  {(1, 1): array([-1.72472853, -0.97666396,  4.70139249]), (1, 0): array([-1.36214078, -0.5005062 ,  3.86264698]), (0, 0): array([ 2.70766988, -0.34033556, -0.36733432]), (0, 1): array([-1.64877464, -1.0397173 ,  4.68849194])}\n",
      "ave reward of node  2 =  0.046390456567865654\n",
      "policy of of node  2 =  {(0, 0): array([ 2.72071999, -0.54532432, -0.17539568]), (0, 1): array([-1.56080864,  4.25666988, -0.69586124]), (1, 0): array([-1.17092598,  3.65557676, -0.48465078]), (1, 1): array([-1.46806625,  4.0407271 , -0.57266085])}\n",
      "ave reward of node  3 =  0.12294271892226587\n",
      "policy of of node  3 =  {(0, 0): array([ 3.54434053, -0.97267928, -0.57166125]), (0, 1): array([-1.70335793, -1.46758861,  5.17094655]), (1, 1): array([-1.3024143 , -1.32883875,  4.63125306]), (1, 0): array([-1.34932387, -1.1063509 ,  4.45567478])}\n",
      "ave reward of node  4 =  0.0733669623817825\n",
      "policy of of node  4 =  {(0, 1): array([-1.62363733,  4.87900475, -1.25536743]), (1, 1): array([-1.05387579,  3.9320703 , -0.87819451]), (1, 0): array([-1.21605038,  4.0639461 , -0.84789571]), (0, 0): array([ 3.73129702, -0.85952229, -0.87177473])}\n",
      "ave reward of node  5 =  0.2674610288985292\n",
      "policy of of node  5 =  {(1, 1): array([-1.87421263,  3.87421263]), (1, 0): array([-1.69822887,  3.69822887]), (0, 1): array([-2.12153966,  4.12153966]), (0, 0): array([ 3.04511126, -1.04511126])}\n",
      "ave reward of node  6 =  0.09176471144469296\n",
      "policy of of node  6 =  {(1, 0): array([-1.62926471,  3.73447338, -0.10520867]), (0, 1): array([-1.85424897,  4.48654713, -0.63229816]), (0, 0): array([ 3.37941925, -0.81295573, -0.56646352]), (1, 1): array([-1.45251465,  3.83049474, -0.37798009])}\n",
      "ave reward of node  7 =  0.07079482010762371\n",
      "policy of of node  7 =  {(0, 1): array([-2.37768657, -1.3618345 , -2.17246187, -1.73048058,  9.64246352]), (1, 1): array([-2.30622683, -2.52226171, -2.57519457, -1.88609212, 11.28977522]), (1, 0): array([-2.12931553, -2.44218183,  9.60227076, -1.93195776, -1.09881564]), (0, 0): array([-2.73374285, -0.91313389, -1.57241746, -1.10169644,  8.32099064])}\n",
      "ave reward of node  8 =  0.9085980870556611\n",
      "policy of of node  8 =  {(1, 1): array([-1.09142144, -1.27842235,  6.29339366, -0.66970016, -1.25384971]), (0, 1): array([-1.08728228, -1.32032578,  8.4092252 , -1.6315374 , -2.37007975]), (0, 0): array([ 6.0906345 , -1.32324372, -1.10994246, -1.32967469, -0.32777363]), (1, 0): array([ 1.74686824,  1.81375972, -0.55173675, -1.05353144,  0.04464023])}\n",
      "ave reward of node  9 =  0.05106995147026127\n",
      "policy of of node  9 =  {(1, 1): array([-0.79835181, -0.52159508,  1.59452467, -0.51237344,  2.23779566]), (1, 0): array([-1.08939528, -0.63133535,  2.4513271 , -0.56035621,  1.82975973]), (0, 0): array([ 3.23435931, -0.65816814, -0.30966208, -0.09204712, -0.17448196]), (0, 1): array([-1.10001238, -1.02319577,  0.66863139, -1.2187992 ,  4.67337596])}\n",
      "ave reward of node  10 =  0.3205505131559007\n",
      "policy of of node  10 =  {(1, 1): array([-1.23591461, -1.2694832 , -1.05605466,  6.62750168, -1.06604922]), (0, 0): array([ 3.82479278, -0.67082742, -0.38680623, -0.34935233, -0.4178068 ]), (0, 1): array([-1.34604088, -0.90396788, -1.26077219,  6.57801326, -1.06723231]), (1, 0): array([-0.86211579, -0.61158448, -0.83694939,  5.2922912 , -0.98164154])}\n",
      "ave reward of node  11 =  0.05028479237918535\n",
      "policy of of node  11 =  {(0, 1): array([-1.41841341, -0.22920288,  3.64761629]), (1, 0): array([-1.49597204,  1.17859204,  2.31738   ]), (1, 1): array([-1.68350855,  1.6961797 ,  1.98732885]), (0, 0): array([ 2.91212408, -0.33411396, -0.57801012])}\n",
      "ave reward of node  12 =  0.3233340300077411\n",
      "policy of of node  12 =  {(0, 1): array([-1.82684672,  5.32871958, -1.50187286]), (1, 1): array([-1.53758029,  4.85247941, -1.31489913]), (0, 0): array([ 4.04513787, -1.0355313 , -1.00960657]), (1, 0): array([-1.39221628,  4.62189943, -1.22968315])}\n",
      "ave reward of node  13 =  0.0746403183719074\n",
      "policy of of node  13 =  {(1, 1): array([-2.60595246, -2.70352955, 12.36488808, -2.52676758, -2.52863849]), (0, 0): array([-2.16386356, -1.76248498,  8.95727976, -1.40934216, -1.62158906]), (0, 1): array([-2.48284371, -1.79815968, 10.95586572, -2.45245876, -2.22240357]), (1, 0): array([10.37935598, -1.86077509, -2.62169539, -2.16901356, -1.72787194])}\n",
      "ave reward of node  14 =  0.004909400920557176\n",
      "policy of of node  14 =  {(0, 0): array([ 2.2328916 , -0.12274408, -0.07909103,  0.03400233, -0.06505883]), (0, 1): array([ 0.08347473, -0.40588282,  0.28726534,  1.76217547,  0.27296728]), (1, 0): array([-0.13657108, -0.45739076,  0.18765628,  2.46394755, -0.057642  ]), (1, 1): array([ 1.835375  , -0.0351016 ,  0.05932295,  0.08883598,  0.05156767])}\n",
      "ave reward of node  15 =  0.009615011908943403\n",
      "policy of of node  15 =  {(0, 0): array([ 2.98869169, -0.01857556, -0.43792743, -0.47080478, -0.06138392]), (0, 1): array([-0.6544006 , -0.38635722, -0.32549213, -0.26126627,  3.62751622]), (1, 0): array([-0.37524886, -0.31401099, -0.14993785, -0.142371  ,  2.98156871]), (1, 1): array([ 1.59208663,  0.10744772, -0.05347624,  0.11402198,  0.2399199 ])}\n",
      "ave reward of node  16 =  0.0014244411368004683\n",
      "policy of of node  16 =  {(0, 0): array([ 1.80073932,  0.06797907, -0.06782216,  0.10896049,  0.09014329]), (0, 1): array([ 0.86793873, -0.06649526,  0.04687822,  0.7681724 ,  0.3835059 ]), (1, 0): array([ 1.18565811,  0.05324182, -0.08417382,  0.57675578,  0.26851811]), (1, 1): array([ 1.96795132, -0.01402429, -0.0043885 ,  0.0352435 ,  0.01521798])}\n",
      "ave reward of node  17 =  0.23906724728723663\n",
      "policy of of node  17 =  {(0, 1): array([-3.46403113,  8.56579456, -3.10176343]), (1, 1): array([-3.33621186,  8.79036483, -3.45415297]), (1, 0): array([-2.25395014,  7.61693439, -3.36298425]), (0, 0): array([-3.30444645, -2.28044315,  7.5848896 ])}\n",
      "ave reward of node  18 =  0.30238437930856654\n",
      "policy of of node  18 =  {(0, 1): array([-1.73106727, -1.33701372,  5.06808099]), (1, 1): array([-1.69769505, -1.49641649,  5.19411154]), (1, 0): array([-1.36841773, -1.01431514,  4.38273287]), (0, 0): array([ 3.28420901, -0.54176877, -0.74244023])}\n",
      "ave reward of node  19 =  0.21225965143460723\n",
      "policy of of node  19 =  {(1, 1): array([-2.96740045, 12.12351283, -2.29737863, -2.55550027, -2.30323349]), (0, 1): array([-3.02879904, -1.60463015, 11.56690872, -2.85439661, -2.07908292]), (1, 0): array([-2.43616212, -1.36151115, 10.37870228, -1.99636174, -2.58466726]), (0, 0): array([-2.68777   , -1.8740791 ,  9.81241829, -1.91138862, -1.33918057])}\n",
      "ave reward of node  20 =  0.9663232870760926\n",
      "policy of of node  20 =  {(1, 1): array([-1.23973577, -0.82365977, -1.48858941,  7.23677834, -1.68479339]), (0, 1): array([-1.204941  , -2.37791557, -2.19158884,  8.57778067, -0.80333526]), (1, 0): array([-0.31118706, -0.04282957,  0.3521776 ,  2.33854777, -0.33670874]), (0, 0): array([ 5.83127171, -0.65546253, -0.90994482, -1.22501623, -1.04084812])}\n",
      "ave reward of node  21 =  0.2818609016503615\n",
      "policy of of node  21 =  {(1, 1): array([-2.25063178,  9.80452615, -1.61738924, -1.95297425, -1.98353088]), (1, 0): array([-1.96298679,  8.55877724, -1.69203681, -1.96741761, -0.93633602]), (0, 1): array([-2.41615622,  9.88428182, -2.06134534, -1.49347077, -1.91330948]), (0, 0): array([-3.37238696, -1.57842963, -0.37702847,  8.54942927, -1.22158422])}\n",
      "ave reward of node  22 =  0.2227227487881658\n",
      "policy of of node  22 =  {(0, 0): array([ 4.38934106, -0.77758728, -0.33664234, -0.61170123, -0.66341021]), (0, 1): array([-1.34057493,  6.00171903, -0.77011602, -0.9034965 , -0.98753157]), (1, 0): array([-1.01021957,  5.32640287, -0.62465287, -0.88016719, -0.81136323]), (1, 1): array([-0.6728885 ,  5.24362268, -0.77486746, -0.82362081, -0.97224591])}\n",
      "ave reward of node  23 =  0.4936044633364035\n",
      "policy of of node  23 =  {(1, 0): array([-1.14257485,  4.43818123, -1.29560638]), (0, 1): array([-1.90966976,  5.68717216, -1.7775024 ]), (1, 1): array([-1.660764  ,  5.26560432, -1.60484032]), (0, 0): array([ 4.39600712, -1.19057527, -1.20543184])}\n",
      "ave reward of node  24 =  0.012047764358206038\n",
      "policy of of node  24 =  {(0, 0): array([ 2.11508713, -0.02154076, -0.09354637]), (0, 1): array([-0.81955331,  1.10812446,  1.71142885]), (1, 0): array([-0.39335107,  1.7323412 ,  0.66100988]), (1, 1): array([1.66185759, 0.12088222, 0.21726019])}\n",
      "ave reward of node  25 =  0.2341973916188242\n",
      "policy of of node  25 =  {(1, 0): array([-0.92509729, -0.63346334, -0.97530938, -0.5355984 ,  5.06946842]), (0, 1): array([-1.34737584, -0.59225919, -0.81509022, -0.80288189,  5.55760715]), (1, 1): array([-1.44658771, -0.53362953, -1.27156418, -0.56978479,  5.82156621]), (0, 0): array([ 3.34115809, -0.65547969, -0.01043937, -0.2915522 , -0.38368682])}\n",
      "ave reward of node  26 =  0.02799276118018579\n",
      "policy of of node  26 =  {(0, 0): array([ 2.54512532, -0.18663826, -0.02540387, -0.21635074, -0.11673246]), (0, 1): array([-0.81215149, -0.85821192,  3.39860868,  0.14197793,  0.1297768 ]), (1, 0): array([-0.76559731, -0.56077279,  3.05663967,  0.41744403, -0.14771359]), (1, 1): array([ 1.30213373,  0.15494888,  0.43543053,  0.12810912, -0.02062226])}\n",
      "ave reward of node  27 =  0.14002225202814658\n",
      "policy of of node  27 =  {(1, 1): array([-2.604558  , -2.79833355, 12.98094874, -2.71409571, -2.86396149]), (0, 0): array([-2.35866343, -1.65011363, -1.72840461,  9.70844244, -1.97126077]), (0, 1): array([11.95598941, -2.57962038, -2.29548206, -2.63926069, -2.44162628]), (1, 0): array([10.7384694 , -1.45313405, -2.23937171, -2.75894047, -2.28702318])}\n",
      "ave reward of node  28 =  0.13726585790109386\n",
      "policy of of node  28 =  {(1, 1): array([-1.07643397, -0.60264286, -0.74454686,  5.24995527, -0.82633157]), (1, 0): array([-1.03733883, -0.593184  , -0.70825864,  5.24795001, -0.90916854]), (0, 0): array([ 4.20765542, -0.43241894, -0.50334128, -0.53359644, -0.73829876]), (0, 1): array([-1.08472183, -0.67998245, -0.79306246,  5.71222194, -1.15445519])}\n",
      "ave reward of node  29 =  0.08858606897044657\n",
      "policy of of node  29 =  {(0, 1): array([-1.29895931,  4.42138602, -1.12242671]), (1, 0): array([-1.23665846,  4.18392946, -0.947271  ]), (0, 0): array([ 2.93645496, -0.60887772, -0.32757724]), (1, 1): array([-0.94998124,  3.74569405, -0.79571281])}\n",
      "ave reward of node  30 =  0.02808848453837595\n",
      "policy of of node  30 =  {(0, 0): array([ 2.32876475, -0.32876475]), (0, 1): array([-1.42510042,  3.42510042]), (1, 0): array([-0.83410895,  2.83410895]), (1, 1): array([-0.51351584,  2.51351584])}\n",
      "ave reward of node  31 =  0.26787150107244556\n",
      "policy of of node  31 =  {(0, 1): array([-1.6752227 ,  5.34737668, -1.67215398]), (1, 1): array([-1.75774154,  5.33406216, -1.57632061]), (0, 0): array([ 2.73466532, -0.47994381, -0.25472151]), (1, 0): array([-1.23913565,  4.40648928, -1.16735363])}\n",
      "ave reward of node  32 =  0.06534339248366605\n",
      "policy of of node  32 =  {(0, 0): array([ 3.07042566, -0.41925685, -0.65116881]), (0, 1): array([-1.57590442, -0.87479849,  4.45070291]), (1, 0): array([-1.32105615, -0.2710645 ,  3.59212065]), (1, 1): array([-1.16048359, -0.66312185,  3.82360544])}\n",
      "ave reward of node  33 =  0.13725525958539872\n",
      "policy of of node  33 =  {(0, 1): array([-1.6664732 ,  4.64056956, -0.97409636]), (1, 1): array([-1.41155138,  4.25554324, -0.84399186]), (1, 0): array([-1.29753935,  4.13327037, -0.83573103]), (0, 0): array([ 2.87000268, -0.28606643, -0.58393625])}\n",
      "ave reward of node  34 =  0.0008886285502542902\n",
      "policy of of node  34 =  {(0, 0): array([ 2.06951632, -0.0031191 , -0.06639722]), (0, 1): array([1.35462485, 0.48822428, 0.15715086]), (1, 0): array([1.50259914, 0.40290037, 0.09450049]), (1, 1): array([ 2.00044857e+00, -2.63539128e-04, -1.85034750e-04])}\n",
      "ave reward of node  35 =  0.11490476353976294\n",
      "policy of of node  35 =  {(1, 1): array([-1.80972931,  3.80972931]), (1, 0): array([-1.54229431,  3.54229431]), (0, 1): array([-1.61054672,  3.61054672]), (0, 0): array([1.65378275, 0.34621725])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 297992/300000 [41:46<00:18, 110.61it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "policyRewardSmoothList = []\n",
    "policyRewardMuList = []\n",
    "finalRewardList = []\n",
    "aveRewardMu = []\n",
    "for k in range(minK,maxK+1):\n",
    "    policyRewardSmooth, policyRewardMu = networkRLModel.train(k = k, M = M, evalInterval = evalInterval,restartIntervalQ = restartIntervalQ\\\n",
    "                                 , restartIntervalPolicy = restartIntervalPolicy, clearPolicy = True)\n",
    "    tmp = 0.0\n",
    "    for i in range(nodeNum):\n",
    "        print('ave reward of node ',str(i), '= ',str(networkRLModel.nodeList[i].mu))\n",
    "        print('policy of of node ', str(i), '= ',networkRLModel.nodeList[i].paramsDict)\n",
    "        tmp+=networkRLModel.nodeList[i].mu\n",
    "    aveRewardMu.append(tmp/nodeNum)\n",
    "    policyRewardSmoothList.append(policyRewardSmooth)\n",
    "    policyRewardMuList.append(policyRewardMu)\n",
    "    finalRewardList.append(policyRewardSmooth[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)  \n",
    "\n",
    "\n",
    "print('average reward calculated by mu = ', aveRewardMu)\n",
    "\n",
    "print('average reward calculated by averaging = ', finalRewardList)\n",
    "legendStr = []\n",
    "for kInd in range(minK,maxK+1):\n",
    "    plt.plot(np.linspace(1,M,len(policyRewardSmooth)), policyRewardSmoothList[kInd-minK])\n",
    "    if kInd == 0:\n",
    "        legendStr.append('Independent Learner')\n",
    "    else:\n",
    "        legendStr.append('Scalable Actor Critic')\n",
    "\n",
    "plt.plot(np.linspace(1,M,len(policyRewardSmooth)), benchMarkReward*np.ones(len(policyRewardSmooth),dtype = float), linestyle = ':')\n",
    "legendStr.append('Benchmark')\n",
    "plt.xlim(0,M)\n",
    "plt.ylim(0.05,.4)\n",
    "plt.legend(legendStr)\n",
    "plt.ylabel('Average Reward')\n",
    "plt.xlabel('$t$')\n",
    "#plt.savefig('neuripsfinal_grid36_k01.png',bbox_inches='tight')\n",
    "\n",
    "# plt.show()\n",
    "# for kInd in range(maxK+1):\n",
    "#     plt.plot(np.linspace(1,M,len(policyRewardMu)), policyRewardMuList[kInd])\n",
    "#     legendStr.append('$\\kappa = '+str(kInd)+'$')\n",
    "# plt.legend(legendStr)\n",
    "#plt.xlim((M/2,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
